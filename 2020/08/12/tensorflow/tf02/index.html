<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>tensorflow2.0与深度学习入门 | 司机站</title><meta name="description" content="全连接层Layers:  input hidden output  Heroes:  BigDATA ReLU Dropout BatchNorm ResNet XAVIR Initialization Caffe&#x2F;Tensorflow&#x2F;PyTorch   tf.keras.layers.Dense(units,activation)layer.Dense类:  .build(num)方法  nu"><meta name="keywords" content="神经网络,深度学习入门,tensorflow"><meta name="author" content="prometheus-code"><meta name="copyright" content="prometheus-code"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://prometheus-code.github.io/2020/08/12/tensorflow/tf02/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="tensorflow2.0与深度学习入门"><meta property="og:url" content="https://prometheus-code.github.io/2020/08/12/tensorflow/tf02/"><meta property="og:site_name" content="司机站"><meta property="og:description" content="全连接层Layers:  input hidden output  Heroes:  BigDATA ReLU Dropout BatchNorm ResNet XAVIR Initialization Caffe&#x2F;Tensorflow&#x2F;PyTorch   tf.keras.layers.Dense(units,activation)layer.Dense类:  .build(num)方法  nu"><meta property="og:image" content="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1597325218551&amp;di=c155f06387bf214ef2017c6db8da3e20&amp;imgtype=0&amp;src=http%3A%2F%2Fwww.wdshouji.com%2Fwp%2Fwp-content%2Fuploads%2F2019%2F03%2F0a67a5c11f1647bfd324fa0d0f7faab5.jpg"><meta property="article:published_time" content="2020-08-12T02:59:34.000Z"><meta property="article:modified_time" content="2020-09-04T07:31:47.385Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="prev" title="tensorflow2.0基础与进阶" href="https://prometheus-code.github.io/2020/08/12/tensorflow/tf01/"><link rel="next" title="改善深层神经网络：超参数调试、正则化以及优化" href="https://prometheus-code.github.io/2020/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true
  }</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.1"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">34</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">14</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">17</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li><li><a class="site-page" href="/categories/novel/"><i class="fa-fw fas fa-book"></i><span> novel</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#全连接层"><span class="toc-number">1.</span> <span class="toc-text">全连接层</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#tf-keras-layers-Dense-units-activation"><span class="toc-number">1.1.</span> <span class="toc-text">tf.keras.layers.Dense(units,activation)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络"><span class="toc-number">1.2.</span> <span class="toc-text">神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-keras-Sequential-layer1-layer2-…"><span class="toc-number">1.2.1.</span> <span class="toc-text">tf.keras.Sequential([layer1,layer2,…])</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#激活函数"><span class="toc-number">2.</span> <span class="toc-text">激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#常见的激活函数"><span class="toc-number">2.1.</span> <span class="toc-text">常见的激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Sigmoid函数"><span class="toc-number">2.1.1.</span> <span class="toc-text">Sigmoid函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ReLU函数"><span class="toc-number">2.1.2.</span> <span class="toc-text">ReLU函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LeakyReLU"><span class="toc-number">2.1.3.</span> <span class="toc-text">LeakyReLU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tanh函数"><span class="toc-number">2.1.4.</span> <span class="toc-text">Tanh函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#输出层函数"><span class="toc-number">2.2.</span> <span class="toc-text">输出层函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#恒等函数"><span class="toc-number">2.2.1.</span> <span class="toc-text">恒等函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#softmax函数"><span class="toc-number">2.2.2.</span> <span class="toc-text">softmax函数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#误差分析"><span class="toc-number">3.</span> <span class="toc-text">误差分析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#MSE"><span class="toc-number">3.1.</span> <span class="toc-text">MSE</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cross-Entopy-Loss"><span class="toc-number">3.2.</span> <span class="toc-text">Cross Entopy Loss</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#反向传播"><span class="toc-number">4.</span> <span class="toc-text">反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#tf-GradientTape"><span class="toc-number">4.1.</span> <span class="toc-text">tf.GradientTape( )</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Himmelblau函数优化练习"><span class="toc-number">4.2.</span> <span class="toc-text">Himmelblau函数优化练习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Keras-API-for-tensorflow"><span class="toc-number">5.</span> <span class="toc-text">Keras API for tensorflow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#网络层类"><span class="toc-number">5.1.</span> <span class="toc-text">网络层类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#常见内置模型层介绍"><span class="toc-number">5.1.1.</span> <span class="toc-text">常见内置模型层介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#自定义层"><span class="toc-number">5.1.2.</span> <span class="toc-text">自定义层</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#网络类"><span class="toc-number">5.2.</span> <span class="toc-text">网络类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#内置网络容器Sequential"><span class="toc-number">5.2.1.</span> <span class="toc-text">内置网络容器Sequential</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型装配、训练与测试"><span class="toc-number">5.2.2.</span> <span class="toc-text">模型装配、训练与测试</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#自定义网络"><span class="toc-number">5.2.3.</span> <span class="toc-text">自定义网络</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#模型保存和加载"><span class="toc-number">5.3.</span> <span class="toc-text">模型保存和加载</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><div id="web_bg" data-type="photo"></div><header class="post-bg" id="page-header" style="background-image: url(https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1597325218551&amp;di=c155f06387bf214ef2017c6db8da3e20&amp;imgtype=0&amp;src=http%3A%2F%2Fwww.wdshouji.com%2Fwp%2Fwp-content%2Fuploads%2F2019%2F03%2F0a67a5c11f1647bfd324fa0d0f7faab5.jpg)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">司机站</a></span><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li><li><a class="site-page" href="/categories/novel/"><i class="fa-fw fas fa-book"></i><span> novel</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">tensorflow2.0与深度学习入门</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-08-12 10:59:34"><i class="far fa-calendar-alt fa-fw"></i> 发表于 2020-08-12</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-09-04 15:31:47"><i class="fas fa-history fa-fw"></i> 更新于 2020-09-04</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/categories/tensorflow%E5%85%A5%E9%97%A8/">tensorflow入门</a></span></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta__icon"></i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h1 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h1><p>Layers:</p>
<ul>
<li>input</li>
<li>hidden</li>
<li>output</li>
</ul>
<p>Heroes:</p>
<ul>
<li>BigDATA</li>
<li>ReLU</li>
<li>Dropout</li>
<li>BatchNorm</li>
<li>ResNet</li>
<li>XAVIR Initialization</li>
<li>Caffe/Tensorflow/PyTorch</li>
</ul>
<hr>
<h2 id="tf-keras-layers-Dense-units-activation"><a href="#tf-keras-layers-Dense-units-activation" class="headerlink" title="tf.keras.layers.Dense(units,activation)"></a>tf.keras.layers.Dense(units,activation)</h2><p>layer.Dense类:</p>
<ul>
<li><p>.build(num)方法</p>
<blockquote>
<p>num为输入节点数</p>
</blockquote>
</li>
<li><p>.kernel方法</p>
<blockquote>
<p>权值张量𝑾</p>
</blockquote>
</li>
<li><p>.bias方法</p>
<blockquote>
<p>偏置张量𝒃</p>
</blockquote>
</li>
<li><p>.trainable_variables属性</p>
<blockquote>
<p>待优化参数列表</p>
</blockquote>
</li>
<li><p>.non_trainable_variables属性</p>
<blockquote>
<p>所有不需要优化的参数列表</p>
</blockquote>
</li>
<li><p>.trainable属性</p>
<blockquote>
<p>所有内部张量列表</p>
</blockquote>
</li>
</ul>
<blockquote>
<p>通过layer.Dense 类，只需要指定<strong>输出</strong>节点数Units 和激活函数类型activation 即可。需要注意的是，输入节点数会根据第一次运算时的输入shape 确定，同时根据输入、输出节点数自动创建并初始化权值张量𝑾和偏置张量𝒃，因此在新建类Dense 实例时，并不会立即创建权值张量𝑾和偏置张量𝒃，而是需要调用build 函数或者直接进行一次前向计算，才能完成网络参数的创建。其中activation 参数指定当前层的激活函数，可以为常见的激活函数或自定义激活函数，也可以指定为None，即无激活函数。</p>
</blockquote>
<p>我们可以通过类内部的成员名kernel 和bias 来获取权值张量𝑾和偏置张量𝒃对象:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x=tf.random.normal([<span class="number">4</span>,<span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line">f=tf.keras.layers.Dense(<span class="number">10.</span>activation=tf.nn.relu)</span><br><span class="line">out=f(x)<span class="comment"># 用f类实例完成一层全连接层的计算</span></span><br><span class="line">f.kernel <span class="comment">#获取w</span></span><br><span class="line">f.bias <span class="comment">#获取b</span></span><br></pre></td></tr></table></figure>
<hr>
<p>机制:</p>
<blockquote>
<p>利用<strong>网络层类对象</strong>进行前向计算时，只需要调用类的<strong>call</strong>方法即可，即写成f(x)方式便可，它会自动调用类的<strong>call</strong>方法，在<strong>call</strong>方法中会自动调用call 方法，这一设定由TensorFlow 框架自动完成，因此用户只需要将网络层的前向计算逻辑实现在call 方法中即可。对于全连接层类，在call 方法中实现𝜎(𝑿@𝑾 + 𝒃)的运算逻辑，非常简单，最后返回全连接层的输出张量即可。</p>
</blockquote>
<hr>
<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h3 id="tf-keras-Sequential-layer1-layer2-…"><a href="#tf-keras-Sequential-layer1-layer2-…" class="headerlink" title="tf.keras.Sequential([layer1,layer2,…])"></a>tf.keras.Sequential([layer1,layer2,…])</h3><blockquote>
<p>可以将每一个层封装起来,调用大类的前向计算函数一次即可完成所有层的前向计算.</p>
</blockquote>
<ul>
<li><p>.build(num)方法</p>
<blockquote>
<p>使用方法同上</p>
</blockquote>
</li>
<li><p>.summary( )方法</p>
<blockquote>
<p>可以打印出每层的参数列表</p>
</blockquote>
</li>
<li><p>call方法</p>
<blockquote>
<p>实现大类的向前运算，可以直接用model(x)实现</p>
<blockquote>
<p>在假定model=tf.keras.Sequential([layer1,layer2,…])时</p>
</blockquote>
</blockquote>
</li>
</ul>
<hr>
<p>实现代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,Sequential</span><br><span class="line">model=Sequential([</span><br><span class="line">    layers.Dense(<span class="number">20</span>,activation=tf.nn.relu),</span><br><span class="line">    layers.Dense(<span class="number">180</span>,activation=tf.nn.relu),</span><br><span class="line">    layers.Dense(<span class="number">10</span>,activation=tf.nn.relu),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">x=tf.random.normal([<span class="number">4</span>,<span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line">out=model(x)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><h2 id="常见的激活函数"><a href="#常见的激活函数" class="headerlink" title="常见的激活函数"></a>常见的激活函数</h2><h3 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h3><script type="math/tex; mode=display">
y=\frac{1}{1+e^{-x}}</script><blockquote>
<p>它的一个优良特性就是能够把𝑥 ∈ 𝑅的输入“压缩”到𝑥 ∈ (0,1)区间，这个区间的数值在机<br>器学习常用来表示以下意义：</p>
<ul>
<li>概率分布 (0,1)区间的输出和概率的分布范围[0,1]契合，可以通过Sigmoid 函数将输出<br>转译为概率输出</li>
<li>信号强度 一般可以将0~1 理解为某种信号的强度，如像素的颜色强度，1 代表当前通<br>道颜色最强，0 代表当前通道无颜色；抑或代表门控值(Gate)的强度，1 代表当前门控<br>全部开放,0代表关闭</li>
</ul>
</blockquote>
<ul>
<li>tf.nn.sigmoid(x)</li>
</ul>
<hr>
<h3 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h3><p>公式:$ReLU(𝑥) ≜ max(0, 𝑥)$</p>
<ul>
<li>tf.nn.relu(x)</li>
</ul>
<hr>
<h3 id="LeakyReLU"><a href="#LeakyReLU" class="headerlink" title="LeakyReLU"></a>LeakyReLU</h3><script type="math/tex; mode=display">
LeakyReLU ≜ \cases{x\quad(x\geq0)\\px\quad(x<0)}</script><ul>
<li>tf.nn.leaky_relu(x, alpha= )</li>
</ul>
<hr>
<h3 id="Tanh函数"><a href="#Tanh函数" class="headerlink" title="Tanh函数"></a>Tanh函数</h3><blockquote>
<p>Tanh 函数能够将𝑥 ∈ 𝑅的输入“压缩”到(−1,1)区间</p>
</blockquote>
<script type="math/tex; mode=display">
tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}</script><ul>
<li>tf.nn.tanh(x)</li>
</ul>
<hr>
<h2 id="输出层函数"><a href="#输出层函数" class="headerlink" title="输出层函数"></a>输出层函数</h2><h3 id="恒等函数"><a href="#恒等函数" class="headerlink" title="恒等函数"></a>恒等函数</h3><p>tf不对输出层处理即可</p>
<blockquote>
<p>回归问题</p>
</blockquote>
<hr>
<h3 id="softmax函数"><a href="#softmax函数" class="headerlink" title="softmax函数"></a>softmax函数</h3><blockquote>
<p>分类问题</p>
</blockquote>
<script type="math/tex; mode=display">
𝑆𝑜𝑓𝑡𝑚 𝑥(𝑧𝑖) ≜ \frac{e^{z_i}}{\sum_{j=1}^{d_{out}}e^{z_j}}</script><ul>
<li>tf.nn.softmax</li>
</ul>
<blockquote>
<p>在 Softmax 函数的数值计算过程中，容易因输入值偏大发生数值溢出现象；在计算交叉熵时，也会出现数值溢出的问题。为了数值计算的稳定性，TensorFlow 中提供了一个统一的接口，将Softmax 与交叉熵损失函数同时实现，同时也处理了数值不稳定的异常，一般推荐使用这些接口函数，避免分开使用Softmax 函数与交叉熵损失函数。</p>
<p>函数式接口为tf.keras.losses.categorical_crossentropy(y_true,y_pred,from_logits=False)，其中y_true 代表了One-hot 编码后的真实标签，y_pred 表示网络的预测值，当from_logits 设置为True 时，y_pred 表示须为未经过Softmax 函数的变量z；</p>
<p>当from_logits 设置为False 时，y_pred 表示为经过Softmax 函数的输出。为了数值计算稳定性，一般设置from_logits 为True，此时tf.keras.losses.categorical_crossentropy 将在内部进行Softmax 函数计算，所以不需要在模型中显式调用Softmax 函数</p>
</blockquote>
<ul>
<li>tf.keras.losses.categorical_crossentropy(y_true,y_pred,from_logits=False)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line">z = tf.random.normal([<span class="number">2</span>,<span class="number">10</span>]) <span class="comment"># 构造输出层的输出</span></span><br><span class="line">print(z.shape)</span><br><span class="line">y_onehot = tf.constant([<span class="number">1</span>,<span class="number">3</span>]) <span class="comment"># 构造真实值</span></span><br><span class="line">y_onehot = tf.one_hot(y_onehot, depth=<span class="number">10</span>) <span class="comment"># one-hot 编码</span></span><br><span class="line">print(y_onehot.shape)</span><br><span class="line"><span class="comment"># 输出层未使用Softmax 函数，故from_logits 设置为True</span></span><br><span class="line"><span class="comment"># 这样categorical_crossentropy 函数在计算损失函数前，会先内部调用Softmax 函数</span></span><br><span class="line">loss = keras.losses.categorical_crossentropy(y_onehot,z,from_logits=<span class="literal">True</span>)</span><br><span class="line">loss = tf.reduce_mean(loss) <span class="comment"># 计算平均交叉熵损失</span></span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure>
<hr>
<blockquote>
<p>6.5 输出层设计<br>我们来特别地讨论网络的最后一层的设计，它除了和所有的隐藏层一样，完成维度变换、特征提取的功能，还作为输出层使用，需要根据具体的任务场景来决定是否使用激活函数，以及使用什么类型的激活函数等。<br>我们将根据输出值的区间范围来分类讨论。常见的几种输出类型包括：<br>❑ 𝑜𝑖 ∈ 𝑅𝑑 输出属于整个实数空间，或者某段普通的实数空间，比如函数值趋势的预<br>测，年龄的预测问题等。<br>❑ 𝑜𝑖 ∈ [0,1] 输出值特别地落在[0, 1]的区间，如图片生成，图片像素值一般用[0, 1]区间<br>的值表示；或者二分类问题的概率，如硬币正反面的概率预测问题。<br>❑ 𝑜𝑖 ∈ [0, 1], 𝑖 𝑜𝑖 = 1 输出值落在[0,1]的区间，并且所有输出值之和为 1，常见的如<br>多分类问题，如MNIST 手写数字图片识别，图片属于10 个类别的概率之和应为1。<br>❑ 𝑜𝑖 ∈ [−1, 1] 输出值在[-1, 1]之间</p>
</blockquote>
<hr>
<h1 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h1><p>Outline:</p>
<ul>
<li>MSE</li>
<li>Cross Entopy Loss</li>
<li>Hinge Loss</li>
</ul>
<hr>
<h2 id="MSE"><a href="#MSE" class="headerlink" title="MSE"></a>MSE</h2><script type="math/tex; mode=display">
loss=\frac{1}{N}\sum(y-out)^2</script><ul>
<li>tf.losses.MSE(y_true,y_pred)</li>
</ul>
<blockquote>
<p>同时也与tf.reduce_mean( )搭配使用，求出平均误差</p>
</blockquote>
<h2 id="Cross-Entopy-Loss"><a href="#Cross-Entopy-Loss" class="headerlink" title="Cross Entopy Loss"></a>Cross Entopy Loss</h2><ul>
<li>tf.keras.losses.categorical_crossentropy(y_true,y_pred,from_logits=False)</li>
</ul>
<blockquote>
<p>对y_true一定要one_hot化</p>
<p>交叉熵使用的更多</p>
</blockquote>
<h1 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h1><h2 id="tf-GradientTape"><a href="#tf-GradientTape" class="headerlink" title="tf.GradientTape( )"></a>tf.GradientTape( )</h2><p>tensorflow有自动求导的功能，如果遇到一定要手动求导的问题，也可以自己构建计算图的方式来求导</p>
<ul>
<li><p>tf.GradientTape()可以提供求导的上下文管理器来连接需要计算梯度的函数和变量，一般与with as语句连用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x=tf.constant(<span class="number">3.0</span>)</span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    tape.watch(x)</span><br><span class="line">    y=x*x*x</span><br><span class="line">dy_dx=tape.gradient(y,[x])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tf.GradientTape(persistent=True,watch_accessed_variables=True)</p>
<ul>
<li><strong>persistent:</strong> 布尔值，用来指定新创建的gradient tape是否是可持续性的。默认是False，意味着只能够调用一次<code>gradient（）</code>函数。</li>
<li><strong>watch_accessed_variables:</strong> 布尔值，表明这个gradien tap是不是会自动追踪任何能被训练（trainable）的变量。默认是True。要是为False的话，意味着你需要手动去指定你想追踪的那些变量。</li>
</ul>
<blockquote>
<p>但tf.GradientTape只能默认追踪Variable变量，如果是constant变量则需要用.watch( )方法来添加为可追踪变量</p>
</blockquote>
</blockquote>
</li>
</ul>
<h2 id="Himmelblau函数优化练习"><a href="#Himmelblau函数优化练习" class="headerlink" title="Himmelblau函数优化练习"></a>Himmelblau函数优化练习</h2><script type="math/tex; mode=display">
f(x,y)=(x^2+y-11)^2+(x+y^2-7)^2</script><blockquote>
<p>Himmelblau 函数是用来测试优化算法的常用样例函数之一</p>
</blockquote>
<p>用python代码来表示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Himmelblau</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x[<span class="number">0</span>]**<span class="number">2</span>+x[<span class="number">1</span>]<span class="number">-11</span>)**<span class="number">2</span>+(x[<span class="number">0</span>]+x[<span class="number">1</span>]**<span class="number">2</span><span class="number">-7</span>)**<span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>用Matplotlib 库可视化Himmelblau 函数:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(<span class="number">-6</span>, <span class="number">6</span>, <span class="number">0.1</span>) <span class="comment"># 可视化的x 坐标范围为-6~6</span></span><br><span class="line">y = np.arange(<span class="number">-6</span>, <span class="number">6</span>, <span class="number">0.1</span>) <span class="comment"># 可视化的y 坐标范围为-6~6</span></span><br><span class="line">print(<span class="string">'x,y range:'</span>, x.shape, y.shape)</span><br><span class="line"><span class="comment"># 生成x-y 平面采样网格点，方便可视化</span></span><br><span class="line">x, y = np.meshgrid(x, y)</span><br><span class="line">print(<span class="string">'X,Y maps:'</span>, X.shape, Y.shape)</span><br><span class="line">z = Himmelblau([x, y]) <span class="comment"># 计算网格点上的函数值</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"></span><br><span class="line">fig = plt.figure(<span class="string">'himmelblau'</span>)</span><br><span class="line">ax = fig.gca(projection=<span class="string">'3d'</span>) <span class="comment"># 设置3D 坐标轴</span></span><br><span class="line">ax.plot_surface(x, y, z) <span class="comment"># 3D 曲面图</span></span><br><span class="line">ax.view_init(<span class="number">60</span>, <span class="number">-30</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'x'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'y'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>自动求导:</p>
<ul>
<li>tf.constant版</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数的初始化值对优化的影响不容忽视，可以通过尝试不同的初始化值，</span></span><br><span class="line"><span class="comment"># 检验函数优化的极小值情况</span></span><br><span class="line"><span class="comment"># [1., 0.], [-4, 0.], [4, 0.]</span></span><br><span class="line">x = tf.constant([<span class="number">4.</span>, <span class="number">0.</span>]) <span class="comment"># 初始化参数</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">200</span>):<span class="comment"># 循环优化200 次</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape: <span class="comment">#梯度跟踪</span></span><br><span class="line">        tape.watch([x]) <span class="comment"># 加入梯度跟踪列表</span></span><br><span class="line">        y = himmelblau(x) <span class="comment"># 前向传播</span></span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    grads = tape.gradient(y, [x])[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 更新参数,0.01 为学习率</span></span><br><span class="line">    x -= <span class="number">0.01</span>*grads</span><br><span class="line">    <span class="comment"># 打印优化的极小值</span></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">20</span> == <span class="number">19</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'step &#123;&#125;: x = &#123;&#125;, f(x) = &#123;&#125;'</span>.format(step, x.numpy(), y.numpy()))</span><br></pre></td></tr></table></figure>
<ul>
<li>tf.Variable版</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数的初始化值对优化的影响不容忽视，可以通过尝试不同的初始化值，</span></span><br><span class="line"><span class="comment"># 检验函数优化的极小值情况</span></span><br><span class="line"><span class="comment"># [1., 0.], [-4, 0.], [4, 0.]</span></span><br><span class="line">x = tf.Variable([<span class="number">4.</span>, <span class="number">0.</span>]) <span class="comment"># 初始化参数</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">200</span>):<span class="comment"># 循环优化200 次</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape: <span class="comment">#梯度跟踪</span></span><br><span class="line">        tape.watch([x]) <span class="comment"># 加入梯度跟踪列表</span></span><br><span class="line">        y = Himmelblau(x) <span class="comment"># 前向传播</span></span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    grads = tape.gradient(y,[x,])</span><br><span class="line">    <span class="comment"># 更新参数,0.01 为学习率</span></span><br><span class="line">    x.assign_sub(<span class="number">0.01</span>*grads[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 打印优化的极小值</span></span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">20</span> == <span class="number">19</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'step &#123;&#125;: x = &#123;&#125;, f(x) = &#123;&#125;'</span>.format(step, x.numpy(), y.numpy()))</span><br></pre></td></tr></table></figure>
<h1 id="Keras-API-for-tensorflow"><a href="#Keras-API-for-tensorflow" class="headerlink" title="Keras API for tensorflow"></a>Keras API for tensorflow</h1><h2 id="网络层类"><a href="#网络层类" class="headerlink" title="网络层类"></a>网络层类</h2><h3 id="常见内置模型层介绍"><a href="#常见内置模型层介绍" class="headerlink" title="常见内置模型层介绍"></a>常见内置模型层介绍</h3><p>tf.keras.layers下</p>
<p><strong>基础层</strong></p>
<ul>
<li>Dense：密集连接层。参数个数 = 输入层特征数× 输出层特征数(weight)＋ 输出层特征数(bias)</li>
<li>Activation：激活函数层。一般放在Dense层后面，等价于在Dense层中指定activation。</li>
<li>Dropout：随机置零层。训练期间以一定几率将输入置0，一种正则化手段。</li>
<li>BatchNormalization：批标准化层。通过线性变换将输入批次缩放平移到稳定的均值和标准差。可以增强模型对输入不同分布的适应性，加快模型训练速度，有轻微正则化效果。一般在激活函数之前使用。</li>
<li>SpatialDropout2D：空间随机置零层。训练期间以一定几率将整个特征图置0，一种正则化手段，有利于避免特征图之间过高的相关性。</li>
<li>Input：输入层。通常使用Functional API方式构建模型时作为第一层。</li>
<li>DenseFeature：特征列接入层，用于接收一个特征列列表并产生一个密集连接层。</li>
<li>Flatten：压平层，用于将多维张量压成一维。</li>
<li>Reshape：形状重塑层，改变输入张量的形状。</li>
<li>Concatenate：拼接层，将多个张量在某个维度上拼接。</li>
<li>Add：加法层。</li>
<li>Subtract： 减法层。</li>
<li>Maximum：取最大值层。</li>
<li>Minimum：取最小值层。</li>
</ul>
<p><strong>卷积网络相关层</strong></p>
<ul>
<li>Conv1D：普通一维卷积，常用于文本。参数个数 = 输入通道数×卷积核尺寸(如3)×卷积核个数</li>
<li>Conv2D：普通二维卷积，常用于图像。参数个数 = 输入通道数×卷积核尺寸(如3乘3)×卷积核个数</li>
<li>Conv3D：普通三维卷积，常用于视频。参数个数 = 输入通道数×卷积核尺寸(如3乘3乘3)×卷积核个数</li>
<li>SeparableConv2D：二维深度可分离卷积层。不同于普通卷积同时对区域和通道操作，深度可分离卷积先操作区域，再操作通道。即先对每个通道做独立卷积操作区域，再用1乘1卷积跨通道组合操作通道。参数个数 = 输入通道数×卷积核尺寸 + 输入通道数×1×1×输出通道数。深度可分离卷积的参数数量一般远小于普通卷积，效果一般也更好。</li>
<li>DepthwiseConv2D：二维深度卷积层。仅有SeparableConv2D前半部分操作，即只操作区域，不操作通道，一般输出通道数和输入通道数相同，但也可以通过设置depth_multiplier让输出通道为输入通道的若干倍数。输出通道数 = 输入通道数 × depth_multiplier。参数个数 = 输入通道数×卷积核尺寸× depth_multiplier。</li>
<li>Conv2DTranspose：二维卷积转置层，俗称反卷积层。并非卷积的逆操作，但在卷积核相同的情况下，当其输入尺寸是卷积操作输出尺寸的情况下，卷积转置的输出尺寸恰好是卷积操作的输入尺寸。</li>
<li>LocallyConnected2D: 二维局部连接层。类似Conv2D，唯一的差别是没有空间上的权值共享，所以其参数个数远高于二维卷积。</li>
<li>MaxPooling2D: 二维最大池化层。也称作下采样层。池化层无参数，主要作用是降维。</li>
<li>AveragePooling2D: 二维平均池化层。</li>
<li>GlobalMaxPool2D: 全局最大池化层。每个通道仅保留一个值。一般从卷积层过渡到全连接层时使用，是Flatten的替代方案。</li>
<li>GlobalAvgPool2D: 全局平均池化层。每个通道仅保留一个值。</li>
</ul>
<p><strong>循环网络相关层</strong></p>
<ul>
<li>Embedding：嵌入层。一种比Onehot更加有效的对离散特征进行编码的方法。一般用于将输入中的单词映射为稠密向量。嵌入层的参数需要学习。</li>
<li>LSTM：长短记忆循环网络层。最普遍使用的循环网络层。具有携带轨道，遗忘门，更新门，输出门。可以较为有效地缓解梯度消失问题，从而能够适用长期依赖问题。设置return_sequences = True时可以返回各个中间步骤输出，否则只返回最终输出。</li>
<li>GRU：门控循环网络层。LSTM的低配版，不具有携带轨道，参数数量少于LSTM，训练速度更快。</li>
<li>SimpleRNN：简单循环网络层。容易存在梯度消失，不能够适用长期依赖问题。一般较少使用。</li>
<li>ConvLSTM2D：卷积长短记忆循环网络层。结构上类似LSTM，但对输入的转换操作和对状态的转换操作都是卷积运算。</li>
<li>Bidirectional：双向循环网络包装器。可以将LSTM，GRU等层包装成双向循环网络。从而增强特征提取能力。</li>
<li>RNN：RNN基本层。接受一个循环网络单元或一个循环单元列表，通过调用tf.keras.backend.rnn函数在序列上进行迭代从而转换成循环网络层。</li>
<li>LSTMCell：LSTM单元。和LSTM在整个序列上迭代相比，它仅在序列上迭代一步。可以简单理解LSTM即RNN基本层包裹LSTMCell。</li>
<li>GRUCell：GRU单元。和GRU在整个序列上迭代相比，它仅在序列上迭代一步。</li>
<li>SimpleRNNCell：SimpleRNN单元。和SimpleRNN在整个序列上迭代相比，它仅在序列上迭代一步。</li>
<li>AbstractRNNCell：抽象RNN单元。通过对它的子类化用户可以自定义RNN单元，再通过RNN基本层的包裹实现用户自定义循环网络层。</li>
<li>Attention：Dot-product类型注意力机制层。可以用于构建注意力模型。</li>
<li>AdditiveAttention：Additive类型注意力机制层。可以用于构建注意力模型。</li>
<li>TimeDistributed：时间分布包装器。包装后可以将Dense、Conv2D等作用到每一个时间片段上。</li>
</ul>
<h3 id="自定义层"><a href="#自定义层" class="headerlink" title="自定义层"></a>自定义层</h3><p>对于自定义的网络层，至少需要实现初始化<strong>init</strong>方法和前向传播逻辑<strong>call</strong>方法。</p>
<p>如一个没有偏置向量的全连接层:即bias 为0，同时固定激活函数为ReLU 函数。</p>
<p>实现代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDense</span><span class="params">(layer.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,inp_dim,outp_dim)</span>:</span></span><br><span class="line">        super(MyDense,self).__init__()</span><br><span class="line">        self.kernel=self.add_variable(<span class="string">'w'</span>,[inp_dim,outp_dim],trainable=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self,inputs,training=None)</span>:</span></span><br><span class="line">        out=inputs@self.kernel</span><br><span class="line">        out=tf.nn.relu(out)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<blockquote>
<p>自定义类的前向运算逻辑实现在call(inputs, training=None)函数中，其中inputs代表输入，由用户在调用时传入；training 参数用于指定模型的状态：training 为True 时执行训练模式，training 为False 时执行测试模式，默认参数为None，即测试模式。由于全连接层的训练模式和测试模式逻辑一致，此处不需要额外处理。对于部份测试模式和训练模式不一致的网络层，需要根据training 参数来设计需要执行的逻辑。</p>
</blockquote>
<h2 id="网络类"><a href="#网络类" class="headerlink" title="网络类"></a>网络类</h2><h3 id="内置网络容器Sequential"><a href="#内置网络容器Sequential" class="headerlink" title="内置网络容器Sequential"></a>内置网络容器Sequential</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, Sequential</span><br><span class="line">network = Sequential([ <span class="comment"># 封装为一个网络</span></span><br><span class="line">    layers.Dense(<span class="number">3</span>, activation=<span class="literal">None</span>), <span class="comment"># 全连接层，此处不使用激活函数</span></span><br><span class="line">    layers.ReLU(),<span class="comment">#激活函数层</span></span><br><span class="line">    layers.Dense(<span class="number">2</span>, activation=<span class="literal">None</span>), <span class="comment"># 全连接层，此处不使用激活函数</span></span><br><span class="line">    layers.ReLU() <span class="comment">#激活函数层</span></span><br><span class="line">])</span><br><span class="line">x = tf.random.normal([<span class="number">4</span>,<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">out = network(x) <span class="comment"># 输入从第一层开始，逐层传播至输出层，并返回输出层的输出</span></span><br></pre></td></tr></table></figure>
<p>Sequential 容器也可以通过add()方法继续追加新的网络层，实现动态创建网络的功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers, Sequential</span><br><span class="line">layers_num = <span class="number">2</span> <span class="comment"># 堆叠2 次</span></span><br><span class="line">network = Sequential([]) <span class="comment"># 先创建空的网络容器</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(layers_num):</span><br><span class="line">    network.add(layers.Dense(<span class="number">3</span>)) <span class="comment"># 添加全连接层</span></span><br><span class="line">    network.add(layers.ReLU())<span class="comment"># 添加激活函数层</span></span><br><span class="line">network.build(input_shape=(<span class="number">4</span>, <span class="number">4</span>)) <span class="comment"># 创建网络参数</span></span><br><span class="line">network.summary()</span><br></pre></td></tr></table></figure>
<h3 id="模型装配、训练与测试"><a href="#模型装配、训练与测试" class="headerlink" title="模型装配、训练与测试"></a>模型装配、训练与测试</h3><blockquote>
<p>假定有一个全连接模型:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> Sequential,optimizers,losses</span><br><span class="line">network = Sequential([</span><br><span class="line">    layers.Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">    layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">    layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">    layers.Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">    layers.Dense(<span class="number">10</span>)</span><br><span class="line">])</span><br><span class="line">network.build(input_shape=(<span class="number">4</span>, <span class="number">28</span>*<span class="number">28</span>))</span><br><span class="line">network.summary()</span><br></pre></td></tr></table></figure>
</blockquote>
<p>在keras.model类中</p>
<ul>
<li><p>模型装配</p>
<ul>
<li><p>.compile( )方法</p>
<blockquote>
<p>可以指定网络使用的优化器对象，损失函数类型，评价指标</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 采用Adam 优化器，学习率为0.01;采用交叉熵损失函数，包含Softmax</span></span><br><span class="line">network.compile(</span><br><span class="line">    optimizer=optimizers.Adam(lr=<span class="number">0.01</span>),</span><br><span class="line">    loss=losses.CategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">    metrics=[<span class="string">'accuracy'</span>] <span class="comment"># 设置测量指标为准确率</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>模型训练</p>
<ul>
<li><p>.fit( )方法</p>
<blockquote>
<p>模型装配完成后，即可通过fit()函数送入待训练的数据集和验证用的数据集,进行训练</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定训练集为train_db，验证集为val_db,训练5 个epochs，每2 个epoch 验证一次</span></span><br><span class="line"><span class="comment"># 返回训练轨迹信息保存在history 对象中</span></span><br><span class="line">history = network.fit(train_db, epochs=<span class="number">5</span>, validation_data=val_db,</span><br><span class="line">validation_freq=<span class="number">2</span>,callback=[tensorboard])</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>模型测试，推理</p>
<ul>
<li><p>.predict( )方法</p>
<blockquote>
<p>可以进行模型推理，并返回预测结果</p>
</blockquote>
</li>
<li><p>.evaluate( )方法</p>
<blockquote>
<p>可以直接测试指定数据集所有样本，并打印出性能指标</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h3 id="自定义网络"><a href="#自定义网络" class="headerlink" title="自定义网络"></a>自定义网络</h3><blockquote>
<p>Sequential 容器适合于数据按序从第一层传播到第二层，再从第二层传播到第三层，以<br>此规律传播的网络模型。对于复杂的网络结构，例如第三层的输入不仅是第二层的输出，<br>还有第一层的输出，此时使用自定义网络更加灵活。</p>
</blockquote>
<ul>
<li>网络层的创建</li>
<li>前向运算逻辑</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModel</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line"><span class="comment"># 自定义网络类，继承自Model 基类</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    super(MyModel, self).__init__()</span><br><span class="line">    <span class="comment"># 完成网络内需要的网络层的创建工作</span></span><br><span class="line">    self.fc1 = MyDense(<span class="number">28</span>*<span class="number">28</span>, <span class="number">256</span>)</span><br><span class="line">    self.fc2 = MyDense(<span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">    self.fc3 = MyDense(<span class="number">128</span>, <span class="number">64</span>)</span><br><span class="line">    self.fc4 = MyDense(<span class="number">64</span>, <span class="number">32</span>)</span><br><span class="line">    self.fc5 = MyDense(<span class="number">32</span>, <span class="number">10</span>)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None)</span>:</span></span><br><span class="line">    <span class="comment"># 自定义前向运算逻辑</span></span><br><span class="line">    x = self.fc1(inputs)</span><br><span class="line">    x = self.fc2(x)</span><br><span class="line">    x = self.fc3(x)</span><br><span class="line">    x = self.fc4(x)</span><br><span class="line">    x = self.fc5(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="模型保存和加载"><a href="#模型保存和加载" class="headerlink" title="模型保存和加载"></a>模型保存和加载</h2><ul>
<li><p>张量方式</p>
<ul>
<li><p><strong>Model.save_weights(path)</strong></p>
<blockquote>
<p>通过调用<strong>Model.save_weights(path)</strong>方法即可将当前的网络参数保存到path 文件上，</p>
</blockquote>
</li>
<li><p><strong>Model.load_weights(path)</strong></p>
<blockquote>
<p>然后调用网络对象的<strong>load_weights(path)</strong>方法即可将指定的模型文件中保存的张量数值写入<br>到当前网络参数中去</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">prometheus-code</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://prometheus-code.github.io/2020/08/12/tensorflow/tf02/">https://prometheus-code.github.io/2020/08/12/tensorflow/tf02/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://prometheus-code.github.io" target="_blank">司机站</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/">深度学习入门</a><a class="post-meta__tags" href="/tags/tensorflow/">tensorflow</a></div><div class="post_share"><div class="social-share" data-image="https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=1919536160,281918461&amp;fm=26&amp;gp=0.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/08/12/tensorflow/tf01/"><img class="prev-cover" data-src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1597325218551&amp;di=c155f06387bf214ef2017c6db8da3e20&amp;imgtype=0&amp;src=http%3A%2F%2Fwww.wdshouji.com%2Fwp%2Fwp-content%2Fuploads%2F2019%2F03%2F0a67a5c11f1647bfd324fa0d0f7faab5.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">tensorflow2.0基础与进阶</div></div></a></div><div class="next-post pull-right"><a href="/2020/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96/"><img class="next-cover" data-src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1595764414107&amp;di=93ee542045570b12290d1eb21dff2e30&amp;imgtype=0&amp;src=http%3A%2F%2F5b0988e595225.cdn.sohucs.com%2Fimages%2F20180319%2F66179109ed054505ba02b054d711dbd1.jpeg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">改善深层神经网络：超参数调试、正则化以及优化</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/08/12/tensorflow/tf01/" title="tensorflow2.0基础与进阶"><img class="relatedPosts_cover" data-src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1597325218551&di=c155f06387bf214ef2017c6db8da3e20&imgtype=0&src=http%3A%2F%2Fwww.wdshouji.com%2Fwp%2Fwp-content%2Fuploads%2F2019%2F03%2F0a67a5c11f1647bfd324fa0d0f7faab5.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-12</div><div class="relatedPosts_title">tensorflow2.0基础与进阶</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/09/神经网络/与学习相关的技巧/" title="与学习相关的技巧"><img class="relatedPosts_cover" data-src="https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=13114550,2928413146&fm=26&gp=0.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-09</div><div class="relatedPosts_title">与学习相关的技巧</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/12/神经网络/改善深层神经网络：超参数调试、正则化以及优化/" title="改善深层神经网络：超参数调试、正则化以及优化"><img class="relatedPosts_cover" data-src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1595764414107&di=93ee542045570b12290d1eb21dff2e30&imgtype=0&src=http%3A%2F%2F5b0988e595225.cdn.sohucs.com%2Fimages%2F20180319%2F66179109ed054505ba02b054d711dbd1.jpeg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-12</div><div class="relatedPosts_title">改善深层神经网络：超参数调试、正则化以及优化</div></div></a></div><div class="relatedPosts_item"><a href="/2020/07/28/神经网络/感知机/" title="感知机"><img class="relatedPosts_cover" data-src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1595764414107&di=93ee542045570b12290d1eb21dff2e30&imgtype=0&src=http%3A%2F%2F5b0988e595225.cdn.sohucs.com%2Fimages%2F20180319%2F66179109ed054505ba02b054d711dbd1.jpeg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-28</div><div class="relatedPosts_title">感知机</div></div></a></div><div class="relatedPosts_item"><a href="/2020/07/28/神经网络/神经网络/" title="神经网络的向前传播"><img class="relatedPosts_cover" data-src="https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=13114550,2928413146&fm=26&gp=0.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-28</div><div class="relatedPosts_title">神经网络的向前传播</div></div></a></div><div class="relatedPosts_item"><a href="/2020/07/28/神经网络/神经网络-mnist.py注释/" title="神经网络-mnist.py注释"><img class="relatedPosts_cover" data-src="https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=13114550,2928413146&fm=26&gp=0.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-28</div><div class="relatedPosts_title">神经网络-mnist.py注释</div></div></a></div></div></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By prometheus-code</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><button id="readmode" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font_plus" title="放大字体"><i class="fas fa-plus"></i></button><button id="font_minus" title="缩小字体"><i class="fas fa-minus"></i></button><button class="translate_chn_to_cht" id="translateLink" title="简繁转换">繁</button><button id="darkmode" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script></body></html>