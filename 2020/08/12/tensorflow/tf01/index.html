<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>tensorflow2.0基础与进阶 | 司机站</title><meta name="description" content="关于axis轴参数理解坐标轴 假定是二维元素 就是行元素和列元素变化的方向  axis&#x3D;1对列进行操作，axis&#x3D;0对行进行操作  tf.Tensor list np.array tf.Tensor   为了实现gpu加速，所以重新开发了科学计算库  基本类型 scalar: 1.1 vector:[1.1,2.2,3.3] matrix:[[1.1,2.2],[3.3,4.4]] Tensor:"><meta name="keywords" content="神经网络,深度学习入门,tensorflow"><meta name="author" content="prometheus-code"><meta name="copyright" content="prometheus-code"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://prometheus-code.github.io/2020/08/12/tensorflow/tf01/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="tensorflow2.0基础与进阶"><meta property="og:url" content="https://prometheus-code.github.io/2020/08/12/tensorflow/tf01/"><meta property="og:site_name" content="司机站"><meta property="og:description" content="关于axis轴参数理解坐标轴 假定是二维元素 就是行元素和列元素变化的方向  axis&#x3D;1对列进行操作，axis&#x3D;0对行进行操作  tf.Tensor list np.array tf.Tensor   为了实现gpu加速，所以重新开发了科学计算库  基本类型 scalar: 1.1 vector:[1.1,2.2,3.3] matrix:[[1.1,2.2],[3.3,4.4]] Tensor:"><meta property="og:image" content="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1597325218551&amp;di=c155f06387bf214ef2017c6db8da3e20&amp;imgtype=0&amp;src=http%3A%2F%2Fwww.wdshouji.com%2Fwp%2Fwp-content%2Fuploads%2F2019%2F03%2F0a67a5c11f1647bfd324fa0d0f7faab5.jpg"><meta property="article:published_time" content="2020-08-12T02:59:34.000Z"><meta property="article:modified_time" content="2020-08-21T08:11:08.342Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="prev" title="tensorflow2.0与深度学习入门" href="https://prometheus-code.github.io/2020/08/12/tensorflow/tf02/"><link rel="next" title="改善深层神经网络：超参数调试、正则化以及优化" href="https://prometheus-code.github.io/2020/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true
  }</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.1"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">33</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">16</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">14</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li><li><a class="site-page" href="/categories/novel/"><i class="fa-fw fas fa-book"></i><span> novel</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#关于axis轴参数理解"><span class="toc-number">1.</span> <span class="toc-text">关于axis轴参数理解</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#tf-Tensor"><span class="toc-number">2.</span> <span class="toc-text">tf.Tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#基本类型"><span class="toc-number">2.1.</span> <span class="toc-text">基本类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tf-constant"><span class="toc-number">2.2.</span> <span class="toc-text">tf.constant()</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-rank"><span class="toc-number">2.2.1.</span> <span class="toc-text">tf.rank( ) :</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-is-tensor"><span class="toc-number">2.2.2.</span> <span class="toc-text">tf.is_tensor( ):</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-convert-to-tensor"><span class="toc-number">2.2.3.</span> <span class="toc-text">tf.convert_to_tensor( ):</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-cast-x-dtype"><span class="toc-number">2.2.4.</span> <span class="toc-text">tf.cast(x,dtype&#x3D;)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tf-Variable"><span class="toc-number">2.3.</span> <span class="toc-text">tf.Variable</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#创建一个tensor"><span class="toc-number">3.</span> <span class="toc-text">创建一个tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#转换numpy类型"><span class="toc-number">3.1.</span> <span class="toc-text">转换numpy类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#全部初始化为指定值"><span class="toc-number">3.2.</span> <span class="toc-text">全部初始化为指定值</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-zeros"><span class="toc-number">3.2.1.</span> <span class="toc-text">tf.zeros( )</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-ones"><span class="toc-number">3.2.2.</span> <span class="toc-text">tf.ones( )</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-fill-shape-value"><span class="toc-number">3.2.3.</span> <span class="toc-text">tf.fill(shape,value)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#创建已知分布的张量"><span class="toc-number">3.3.</span> <span class="toc-text">创建已知分布的张量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#正态分布"><span class="toc-number">3.3.1.</span> <span class="toc-text">正态分布</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#均匀分布"><span class="toc-number">3.3.2.</span> <span class="toc-text">均匀分布</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#创造序列"><span class="toc-number">3.3.3.</span> <span class="toc-text">创造序列</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#索引与切片"><span class="toc-number">3.4.</span> <span class="toc-text">索引与切片</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Basic-indexing-and-Numpy-style-indexing"><span class="toc-number">3.4.1.</span> <span class="toc-text">Basic indexing and Numpy-style indexing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#start-end-step"><span class="toc-number">3.4.2.</span> <span class="toc-text">start : end :step</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Selective-Indexing"><span class="toc-number">3.4.3.</span> <span class="toc-text">Selective Indexing</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#维度变换"><span class="toc-number">3.5.</span> <span class="toc-text">维度变换</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-reshape-x-new-shape"><span class="toc-number">3.5.1.</span> <span class="toc-text">tf.reshape(x, new_shape)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-expand-dims-x-axis-和-tf-squeeze-x-axis"><span class="toc-number">3.5.2.</span> <span class="toc-text">tf.expand_dims(x,axis) 和 tf.squeeze(x,axis)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-transpose-x-perm"><span class="toc-number">3.5.3.</span> <span class="toc-text">tf.transpose(x,perm)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#广播"><span class="toc-number">3.5.4.</span> <span class="toc-text">广播</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#基本运算"><span class="toc-number">4.</span> <span class="toc-text">基本运算</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#前向传播实战"><span class="toc-number">5.</span> <span class="toc-text">前向传播实战</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#进阶操作"><span class="toc-number">6.</span> <span class="toc-text">进阶操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#数据打乱"><span class="toc-number">6.1.</span> <span class="toc-text">数据打乱</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-random-shuffle-x"><span class="toc-number">6.1.1.</span> <span class="toc-text">tf.random.shuffle(x)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#合并与分割"><span class="toc-number">6.2.</span> <span class="toc-text">合并与分割</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#合并"><span class="toc-number">6.2.1.</span> <span class="toc-text">合并</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#tf-concat-axis"><span class="toc-number">6.2.1.1.</span> <span class="toc-text">tf.concat([,],axis)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tf-stack-axis"><span class="toc-number">6.2.1.2.</span> <span class="toc-text">tf.stack([,],axis)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#分割"><span class="toc-number">6.2.2.</span> <span class="toc-text">分割</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#tf-unstack-axis"><span class="toc-number">6.2.2.1.</span> <span class="toc-text">tf.unstack([,],axis)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tf-split-x-num-or-size-splits-axis"><span class="toc-number">6.2.2.2.</span> <span class="toc-text">tf.split(x,num_or_size_splits,axis)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数据统计"><span class="toc-number">6.3.</span> <span class="toc-text">数据统计</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#向量范数"><span class="toc-number">6.3.1.</span> <span class="toc-text">向量范数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#tf-norm-x-ord-axis"><span class="toc-number">6.3.1.1.</span> <span class="toc-text">tf.norm(x,ord,axis)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#最值，均值，和"><span class="toc-number">6.3.2.</span> <span class="toc-text">最值，均值，和</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#tf-reduce-max-x-axis"><span class="toc-number">6.3.2.1.</span> <span class="toc-text">tf.reduce_max(x,axis)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tf-reduce-min-x-axis"><span class="toc-number">6.3.2.2.</span> <span class="toc-text">tf.reduce_min(x,axis)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tf-reduce-mean-x-axis"><span class="toc-number">6.3.2.3.</span> <span class="toc-text">tf.reduce_mean(x,axis)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tf-reduce-sum-x-axis"><span class="toc-number">6.3.2.4.</span> <span class="toc-text">tf.reduce_sum(x,axis)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tf-argmax-x-axis-和-tf-argmin-x-axis"><span class="toc-number">6.3.2.5.</span> <span class="toc-text">tf.argmax(x,axis) 和 tf.argmin(x,axis)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#张量比较"><span class="toc-number">6.3.3.</span> <span class="toc-text">张量比较</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#tf-equal-a-b-和-tf-math-equal-a-b"><span class="toc-number">6.3.3.1.</span> <span class="toc-text">tf.equal(a,b) 和 tf.math.equal(a,b)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tf-unique-x"><span class="toc-number">6.3.3.2.</span> <span class="toc-text">tf.unique(x)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#张量排序"><span class="toc-number">6.4.</span> <span class="toc-text">张量排序</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-sort-x-direction-’ASCENDING’-和-tf-argsort-x-direction-’ASCENDING’"><span class="toc-number">6.4.1.</span> <span class="toc-text">tf.sort(x,direction&#x3D;’ASCENDING’) 和 tf.argsort(x,direction&#x3D;’ASCENDING’)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-math-top-k-x-k"><span class="toc-number">6.4.2.</span> <span class="toc-text">tf.math.top_k(x,k)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#填充与复制"><span class="toc-number">6.5.</span> <span class="toc-text">填充与复制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-pad-tensor-paddings-mode-’CONSTANT’-name-None"><span class="toc-number">6.5.1.</span> <span class="toc-text">tf.pad( tensor,paddings,mode&#x3D;’CONSTANT’,name&#x3D;None )</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-tile-tensor"><span class="toc-number">6.5.2.</span> <span class="toc-text">tf.tile(tensor,[ , ,])</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-broadcast-to-tensor"><span class="toc-number">6.5.3.</span> <span class="toc-text">tf.broadcast_to(tensor,[ , , ])</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#张量限幅"><span class="toc-number">6.6.</span> <span class="toc-text">张量限幅</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-maximum-tensor-x-和-tf-minimum-tensor-x"><span class="toc-number">6.6.1.</span> <span class="toc-text">tf.maximum(tensor,x) 和 tf.minimum(tensor,x)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-clip-by-value-a-min-max"><span class="toc-number">6.6.2.</span> <span class="toc-text">tf.clip_by_value(a,min,max)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-clip-by-norm-a-number"><span class="toc-number">6.6.3.</span> <span class="toc-text">tf.clip_by_norm(a,number)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-clip-by-global-norm-grads-number"><span class="toc-number">6.6.4.</span> <span class="toc-text">tf.clip_by_global_norm(grads,number)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#高阶操作技巧"><span class="toc-number">6.7.</span> <span class="toc-text">高阶操作技巧</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-where"><span class="toc-number">6.7.1.</span> <span class="toc-text">tf.where</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-scatter-nd-indices-updates-shape"><span class="toc-number">6.7.2.</span> <span class="toc-text">tf.scatter_nd(indices,updates,shape)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-meshgrid-x-y"><span class="toc-number">6.7.3.</span> <span class="toc-text">tf.meshgrid(x,y)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#数据加载-tenserflow预设数据集"><span class="toc-number">7.</span> <span class="toc-text">数据加载(tenserflow预设数据集)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#常用函数"><span class="toc-number">7.1.</span> <span class="toc-text">常用函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数据加载"><span class="toc-number">7.2.</span> <span class="toc-text">数据加载</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#测试张量"><span class="toc-number">7.3.</span> <span class="toc-text">测试张量</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><div id="web_bg" data-type="photo"></div><header class="post-bg" id="page-header" style="background-image: url(https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1597325218551&amp;di=c155f06387bf214ef2017c6db8da3e20&amp;imgtype=0&amp;src=http%3A%2F%2Fwww.wdshouji.com%2Fwp%2Fwp-content%2Fuploads%2F2019%2F03%2F0a67a5c11f1647bfd324fa0d0f7faab5.jpg)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">司机站</a></span><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li><li><a class="site-page" href="/categories/novel/"><i class="fa-fw fas fa-book"></i><span> novel</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">tensorflow2.0基础与进阶</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-08-12 10:59:34"><i class="far fa-calendar-alt fa-fw"></i> 发表于 2020-08-12</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-08-21 16:11:08"><i class="fas fa-history fa-fw"></i> 更新于 2020-08-21</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/categories/tensorflow%E5%85%A5%E9%97%A8/">tensorflow入门</a></span></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta__icon"></i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h1 id="关于axis轴参数理解"><a href="#关于axis轴参数理解" class="headerlink" title="关于axis轴参数理解"></a>关于axis轴参数理解</h1><p>坐标轴</p>
<p>假定是二维元素</p>
<p>就是行元素和列元素变化的方向</p>
<blockquote>
<p>axis=1对列进行操作，axis=0对行进行操作</p>
</blockquote>
<h1 id="tf-Tensor"><a href="#tf-Tensor" class="headerlink" title="tf.Tensor"></a>tf.Tensor</h1><ul>
<li>list</li>
<li>np.array</li>
<li>tf.Tensor</li>
</ul>
<hr>
<p>为了实现gpu加速，所以重新开发了科学计算库</p>
<hr>
<h2 id="基本类型"><a href="#基本类型" class="headerlink" title="基本类型"></a>基本类型</h2><ul>
<li>scalar: 1.1</li>
<li>vector:[1.1,2.2,3.3]</li>
<li>matrix:[[1.1,2.2],[3.3,4.4]]</li>
<li>Tensor:dim&gt;2</li>
</ul>
<blockquote>
<p>在 TensorFlow 中间，为了表达方便，一般把标量、向量、矩阵也统称为张量，不作区分，需要根据张量的维度数或形状自行判断</p>
</blockquote>
<hr>
<ul>
<li><p>int,float,double </p>
<blockquote>
<p>int32,int64,float32,float64</p>
</blockquote>
</li>
<li><p>bool</p>
</li>
<li><p>string</p>
</li>
</ul>
<hr>
<h2 id="tf-constant"><a href="#tf-constant" class="headerlink" title="tf.constant()"></a>tf.constant()</h2><blockquote>
<p>用来创建一个标量</p>
</blockquote>
<p>可以用以下方法:</p>
<ul>
<li>.gpu( ) :使用gpu</li>
<li>.cpu( )  :使用cpu</li>
<li>.numpy( ) :返回当前值的numpy的数据类型</li>
<li>.nidm : 返回维度</li>
<li>.shape :返回形状</li>
</ul>
<hr>
<h3 id="tf-rank"><a href="#tf-rank" class="headerlink" title="tf.rank( ) :"></a>tf.rank( ) :</h3><blockquote>
<p>可以用来查看张量对象的所有信息</p>
</blockquote>
<h3 id="tf-is-tensor"><a href="#tf-is-tensor" class="headerlink" title="tf.is_tensor( ):"></a>tf.is_tensor( ):</h3><blockquote>
<p>用来判断是否是tf.tensor类型</p>
</blockquote>
<h3 id="tf-convert-to-tensor"><a href="#tf-convert-to-tensor" class="headerlink" title="tf.convert_to_tensor( ):"></a>tf.convert_to_tensor( ):</h3><blockquote>
<p>可以把numpy类型转化为tensor类型</p>
</blockquote>
<h3 id="tf-cast-x-dtype"><a href="#tf-cast-x-dtype" class="headerlink" title="tf.cast(x,dtype=)"></a>tf.cast(x,dtype=)</h3><blockquote>
<p>可以转化类型的精度</p>
</blockquote>
<h2 id="tf-Variable"><a href="#tf-Variable" class="headerlink" title="tf.Variable"></a>tf.Variable</h2><blockquote>
<p>创建一个待优化张量</p>
<blockquote>
<p>由于梯度运算会消耗大量的计算资源，而且会自动更新相关参数，对于不需要的优化的张量，如神经网络的输入𝑿，不需要通过tf.Variable 封装；相反，对于需要计算梯度并优化的张量，如神经网络层的𝑾和𝒃，需要通过tf.Variable 包裹以便TensorFlow 跟踪相关梯度信息。</p>
</blockquote>
</blockquote>
<p>比起普通张量多了两个属性【对象属性】:</p>
<ul>
<li><p>name</p>
<blockquote>
<p>name 属性用于命名计算图中的变量，这套命名体系是TensorFlow 内部维护的，一般不需要用户关注name 属性</p>
</blockquote>
</li>
<li><p>trainable</p>
<blockquote>
<p>trainable属性表征当前张量是否需要被优化，创建Variable 对象时是默认启用优化标志，可以设置<br>trainable=False 来设置张量不需要优化。</p>
</blockquote>
</li>
</ul>
<hr>
<p>然而普通张量其实也可以通过GradientTape.watch()方法临时加入跟踪梯度信息的列表，从而支持自动求导功能。</p>
<h1 id="创建一个tensor"><a href="#创建一个tensor" class="headerlink" title="创建一个tensor"></a>创建一个tensor</h1><h2 id="转换numpy类型"><a href="#转换numpy类型" class="headerlink" title="转换numpy类型"></a>转换numpy类型</h2><blockquote>
<p> tf.constant( ) 或 tf.Varible( ) 或 tf.convert_to_tensor( ) </p>
<p> 可以使用这三个函数来转化numpy类型来创建tensor</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.convert_to_tensor(np.ones([<span class="number">2</span>,<span class="number">3</span>]),dtype=tf.float32)</span><br><span class="line"><span class="comment">#&lt;tf.Tensor: id=11, shape=(2, 3), dtype=float32, numpy=</span></span><br><span class="line"><span class="comment">#array([[1., 1., 1.],</span></span><br><span class="line"><span class="comment">#       [1., 1., 1.]], dtype=float32)&gt;</span></span><br><span class="line">tf.Variable(np.ones([<span class="number">2</span>,<span class="number">3</span>]))</span><br><span class="line"><span class="comment">#&lt;tf.Variable 'Variable:0' shape=(2, 3) dtype=float64, numpy=</span></span><br><span class="line"><span class="comment">#array([[1., 1., 1.],</span></span><br><span class="line"><span class="comment">#       [1., 1., 1.]])&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="全部初始化为指定值"><a href="#全部初始化为指定值" class="headerlink" title="全部初始化为指定值"></a>全部初始化为指定值</h2><h3 id="tf-zeros"><a href="#tf-zeros" class="headerlink" title="tf.zeros( )"></a>tf.zeros( )</h3><p>创建全0张量</p>
<blockquote>
<p>tf.zeros([ ])</p>
<p>传入[1,2]类似np.zeros((1,2))或np.zeros([1,2])</p>
</blockquote>
<ul>
<li><p>tf.zeros_like( )</p>
<blockquote>
<p>与np.zeros_like( )类似</p>
</blockquote>
</li>
</ul>
<hr>
<h3 id="tf-ones"><a href="#tf-ones" class="headerlink" title="tf.ones( )"></a>tf.ones( )</h3><p>创建全1 张量</p>
<blockquote>
<p>使用方法同上</p>
</blockquote>
<ul>
<li><p>tf.ones_like( )</p>
<blockquote>
<p>使用方法同上</p>
</blockquote>
</li>
</ul>
<hr>
<h3 id="tf-fill-shape-value"><a href="#tf-fill-shape-value" class="headerlink" title="tf.fill(shape,value)"></a>tf.fill(shape,value)</h3><blockquote>
<p>除了初始化为全0，或全1 的张量之外，有时也需要全部初始化为某个自定义数值的<br>张量，比如将张量的数值全部初始化为−1等。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.fill([<span class="number">2</span>,<span class="number">2</span>],<span class="number">3</span>)</span><br><span class="line"><span class="comment">#&lt;tf.Tensor: id=13, shape=(2, 2), dtype=int32, numpy=</span></span><br><span class="line"><span class="comment">#array([[3, 3],</span></span><br><span class="line"><span class="comment">#       [3, 3]])&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="创建已知分布的张量"><a href="#创建已知分布的张量" class="headerlink" title="创建已知分布的张量"></a>创建已知分布的张量</h2><h3 id="正态分布"><a href="#正态分布" class="headerlink" title="正态分布"></a>正态分布</h3><p>tf.random.normal(shape, mean=0.0, stddev=1.0)</p>
<blockquote>
<p>可以创建形状为shape，均值为mean，标准差为stddev 的正态分布𝒩($mean, stddev^2$)。</p>
</blockquote>
<ul>
<li><p>tf.random.truncated_normal(shape, mean=0.0, stddev=1.0)</p>
<blockquote>
<p>会截取正态分布的一部分，有利于sigmoid函数的梯度下降</p>
</blockquote>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tf.random.truncated_normal([<span class="number">3</span>,<span class="number">3</span>], mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>)</span><br><span class="line">&lt;tf.Tensor: id=<span class="number">25</span>, shape=(<span class="number">3</span>, <span class="number">3</span>), dtype=float32, numpy=</span><br><span class="line">array([[ <span class="number">0.07241093</span>, <span class="number">-0.9492289</span> ,  <span class="number">1.1106242</span> ],</span><br><span class="line">       [<span class="number">-1.0468991</span> , <span class="number">-0.00725726</span>,  <span class="number">1.6111004</span> ],</span><br><span class="line">       [<span class="number">-1.2311684</span> ,  <span class="number">1.380989</span>  ,  <span class="number">0.62593746</span>]], dtype=float32)&gt;</span><br></pre></td></tr></table></figure>
<h3 id="均匀分布"><a href="#均匀分布" class="headerlink" title="均匀分布"></a>均匀分布</h3><p>tf.random.uniform(shape, minval=0, maxval=None, dtype=tf.float32)</p>
<blockquote>
<p>可以创建采样自[minval, maxval)区间的均匀分布的张量。</p>
</blockquote>
<ul>
<li>如果需要均匀采样整形类型的数据，必须指定采样区间的最大值maxval 参数，同时指定数据类型为tf.int*型</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.random.uniform([<span class="number">3</span>,<span class="number">3</span>],minval=<span class="number">1</span>,maxval=<span class="number">100</span>)</span><br><span class="line"><span class="comment">#&lt;tf.Tensor: id=32, shape=(3, 3), dtype=float32, numpy=</span></span><br><span class="line"><span class="comment">#array([[77.254425, 22.276836, 78.80216 ],</span></span><br><span class="line"><span class="comment">#      [68.57904 ,  3.394451, 20.12668 ],</span></span><br><span class="line"><span class="comment">#      [93.60507 , 49.33812 , 62.72094 ]], dtype=float32)&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="创造序列"><a href="#创造序列" class="headerlink" title="创造序列"></a>创造序列</h3><p>tf.range(limit,delta=1)</p>
<blockquote>
<p>tf.range(limit, delta=1)可以创建[0, limit)之间，步长为delta 的整型序列，不包含limit 本身。</p>
</blockquote>
<hr>
<p>Scalar:</p>
<ul>
<li>[ ]</li>
<li>loss</li>
<li>accuracy</li>
</ul>
<p>Vector:</p>
<ul>
<li>Bias</li>
<li>[out_dim]</li>
</ul>
<p>Matrix:</p>
<ul>
<li>input x:[b,vec_dim]</li>
<li>weight:[input_dim,output_dim]</li>
</ul>
<p>Dim=3 Tensor:</p>
<ul>
<li>x: [b,seq_len,word_dim]</li>
</ul>
<blockquote>
<p>一般将单词通过嵌入层(Embedding Layer)编码为固定长度的向量，比如“a”编码为某个长度3 的向量，那么2 个等长(单词数量为5)的句子序列可以表示为shape 为[2,5,3]的3 维张量，其中2 表示句子个数，5 表示单词数量，3 表示单词向量的长度。</p>
</blockquote>
<p>Dim=4 Tensor</p>
<ul>
<li>Image:[b,h,w,3]</li>
<li>feature maps:[b,h,w,c]</li>
</ul>
<blockquote>
<p>图片相关的处理</p>
</blockquote>
<p>Dim=5 Tensor</p>
<ul>
<li>Single task: [b,h,w,3]</li>
<li>meta-learning: [task_b,b,h,w,3]</li>
</ul>
<h2 id="索引与切片"><a href="#索引与切片" class="headerlink" title="索引与切片"></a>索引与切片</h2><h3 id="Basic-indexing-and-Numpy-style-indexing"><a href="#Basic-indexing-and-Numpy-style-indexing" class="headerlink" title="Basic indexing and Numpy-style indexing"></a>Basic indexing and Numpy-style indexing</h3><blockquote>
<p>在 TensorFlow 中，支持基本的[𝑖],[𝑗] ⋯标准索引方式，也支持通过逗号分隔索引号的索引方式。</p>
</blockquote>
<p>推荐用x[1,2,3]这种索引方式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x=tf.random.normal([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],mean=<span class="number">0</span>,stddev=<span class="number">1</span>)</span><br><span class="line">x[<span class="number">0</span>]</span><br><span class="line">x[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">x[<span class="number">0</span>][<span class="number">1</span>][<span class="number">2</span>]</span><br><span class="line">x[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<h3 id="start-end-step"><a href="#start-end-step" class="headerlink" title="start : end :step"></a>start : end :step</h3><blockquote>
<p>与numpy中使用方式相同</p>
</blockquote>
<p>为了避免出现像 [: , : , : ,1]这样过多冒号的情况，可以使用⋯符号表示取多个维度上所有的数据，其中维度的数量需根据规则自动推断：当切片方式出现⋯符号时，⋯符号左边的维度将自动对齐到最左边，⋯符号右边的维度将自动对齐到最右边，此时系统再自动推断⋯符号代表的维度数量</p>
<ul>
<li><p>读取第 1~2 张图片的G/B 通道数据，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x[<span class="number">0</span>:<span class="number">2</span>,...,<span class="number">1</span>:]</span><br></pre></td></tr></table></figure>
<p><img src= "/img/loading.gif" data-src="/2020/08/12/tensorflow/tf01/%E5%88%87%E7%89%87.png" style="zoom: 80%;"></p>
</li>
</ul>
<p>特别地，step 可以为负数，考虑最特殊的一种例子，当step = −1时，start: end: −1表示从start 开始，逆序读取至end 结束(不包含end)，索引号𝑒𝑛𝑑 ≤ 𝑠𝑡𝑎𝑟𝑡。考虑一个0~9 的简单序列向量，逆序取到第1 号元素，不包含第1 号</p>
<h3 id="Selective-Indexing"><a href="#Selective-Indexing" class="headerlink" title="Selective Indexing"></a>Selective Indexing</h3><ul>
<li><p>tf.gather(params,indices,axis=0)</p>
<blockquote>
<p>从params的axis维根据indices的参数值顺序获取切片</p>
<blockquote>
<p>只在一个维度中取值</p>
</blockquote>
</blockquote>
<p><img src= "/img/loading.gif" data-src="/2020/08/12/tensorflow/tf01/12389.png" alt></p>
</li>
<li><p>tf.gather_nd(  params,  indices,  name=None )</p>
<blockquote>
<p>这里的idices可以为[[0, 2], [0, 4], [2, 2]]或[[0], [1]]或([0])或[0, 1]</p>
<p>类似坐标轴取数</p>
<blockquote>
<p>有一点不同的地方是[[2,3]]和[2,3]一个返回向量，一个返回标量</p>
</blockquote>
</blockquote>
</li>
<li><p>tf.boolean_mask(  tensor,  mask,  name=’boolean_mask’,   axis=None )</p>
<blockquote>
<p>根据bool类型mask数组来判断是非保留数据</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a=tf.range(<span class="number">5</span>*<span class="number">5</span>*<span class="number">5</span>*<span class="number">5</span>)</span><br><span class="line">a=tf.reshape(a,[<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>])</span><br><span class="line">tf.boolean_mask(a,mask=[<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">True</span>])<span class="comment">#只取0，1，4号第0轴元素</span></span><br><span class="line">tf.boolean_mask(a,mask=[<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>,<span class="literal">True</span>]，axis=<span class="number">3</span>)<span class="comment">#只取第3轴的0，1，4号</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="维度变换"><a href="#维度变换" class="headerlink" title="维度变换"></a>维度变换</h2><p>全连接层的向前传播:</p>
<p>在tensorflow中:$Y=X@W+b$</p>
<hr>
<p>其实都差不多</p>
<h3 id="tf-reshape-x-new-shape"><a href="#tf-reshape-x-new-shape" class="headerlink" title="tf.reshape(x, new_shape)"></a>tf.reshape(x, new_shape)</h3><blockquote>
<p>通过tf.reshape(x, new_shape)，可以将张量的视图任意地合法改变</p>
</blockquote>
<h3 id="tf-expand-dims-x-axis-和-tf-squeeze-x-axis"><a href="#tf-expand-dims-x-axis-和-tf-squeeze-x-axis" class="headerlink" title="tf.expand_dims(x,axis) 和 tf.squeeze(x,axis)"></a>tf.expand_dims(x,axis) 和 tf.squeeze(x,axis)</h3><blockquote>
<p>tf.expand_dims(x, axis)可在指定的axis 轴前可以插入一个新的维度</p>
<blockquote>
<p>需要注意的是，tf.expand_dims 的axis 为正时，表示在当前维度之前插入一个新维度；为<br>负时，表示当前维度之后插入一个新的维度。</p>
</blockquote>
<p>tf.squeeze(x,axis)可删除指定轴</p>
</blockquote>
<h3 id="tf-transpose-x-perm"><a href="#tf-transpose-x-perm" class="headerlink" title="tf.transpose(x,perm)"></a>tf.transpose(x,perm)</h3><blockquote>
<p>perm提供新的轴的读取顺序</p>
</blockquote>
<hr>
<h3 id="广播"><a href="#广播" class="headerlink" title="广播"></a>广播</h3><p>tf.broadcast_to(  input, shape, name=None )</p>
<blockquote>
<p>几乎不会用到，tensorflow里有与numpy类似的广播机制</p>
</blockquote>
<p>将原始矩阵成倍增加</p>
<h1 id="基本运算"><a href="#基本运算" class="headerlink" title="基本运算"></a>基本运算</h1><p>Outline</p>
<ul>
<li><p>+-*/</p>
</li>
<li><p>** ， pow ，square</p>
</li>
<li><p>sqrt</p>
</li>
<li><p>//,%</p>
</li>
<li><p>exp,log</p>
<blockquote>
<p>tf.math.log  #只以e为底</p>
<blockquote>
<p>其他底只能通过对数相除得到</p>
</blockquote>
<p>tf.exp</p>
<blockquote>
<p>tf2.2 exp已经被移到math模块</p>
</blockquote>
</blockquote>
</li>
<li><p>@,matmul</p>
<blockquote>
<p>类似np.dot</p>
</blockquote>
</li>
<li><p>linear layer</p>
</li>
</ul>
<hr>
<p>Operation type</p>
<ul>
<li><p>element-wise(对应元素运算)</p>
<ul>
<li>+-*/</li>
</ul>
</li>
<li><p>matrix-wise</p>
<ul>
<li>@,matmul</li>
</ul>
</li>
<li>dim-wise<ul>
<li>reduce_mean/max/min/sum</li>
</ul>
</li>
</ul>
<h1 id="前向传播实战"><a href="#前向传播实战" class="headerlink" title="前向传播实战"></a>前向传播实战</h1><p>与python+numpy实现方式类似，但tensorflow提供了自动求导函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[1]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># - 可以使用以下方法屏蔽tensorflow无关信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[2]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">'TF_CPP_MIN_LOG_LEVEL'</span>]=<span class="string">'2'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># - 现在加载文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[3]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(x,y),_=datasets.mnist.load_data( )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[4]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x=tf.constant(x,dtype=tf.float32)/<span class="number">255</span></span><br><span class="line">y=tf.constant(y,dtype=tf.int32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># - 现在对数据进行切片并打包</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[5]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x,y)).batch(<span class="number">128</span>)</span><br><span class="line">loss_data=[]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># - 对权重进行初始化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[6]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">w1=tf.Variable(tf.random.truncated_normal([<span class="number">784</span>,<span class="number">256</span>],stddev=<span class="number">0.1</span>))</span><br><span class="line">b1=tf.Variable(tf.zeros([<span class="number">256</span>]))</span><br><span class="line"></span><br><span class="line">w2=tf.Variable(tf.random.truncated_normal([<span class="number">256</span>,<span class="number">128</span>],stddev=<span class="number">0.1</span>))</span><br><span class="line">b2=tf.Variable(tf.zeros([<span class="number">128</span>]))</span><br><span class="line"></span><br><span class="line">w3=tf.Variable(tf.random.truncated_normal([<span class="number">128</span>,<span class="number">10</span>],stddev=<span class="number">0.1</span>))</span><br><span class="line">b3=tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line">lr=<span class="number">1e-3</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># - 向前传播</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[7]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">30</span>):</span><br><span class="line">    <span class="keyword">for</span> step,(x,y) <span class="keyword">in</span> enumerate(train_db):</span><br><span class="line">        x=tf.reshape(x,[<span class="number">-1</span>,<span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            h1= x@w1 + b1</span><br><span class="line">            h1=tf.nn.relu(h1)</span><br><span class="line"></span><br><span class="line">            h2= h1@w2 + b2</span><br><span class="line">            h2=tf.nn.relu(h2)</span><br><span class="line"></span><br><span class="line">            out= h2@w3 +b3</span><br><span class="line"></span><br><span class="line">            y_onehot=tf.one_hot(y,depth=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">            loss=tf.square(y_onehot - out)</span><br><span class="line">            loss=tf.reduce_mean(loss)</span><br><span class="line"></span><br><span class="line">        grads=tape.gradient(loss,[w1,b1,w2,b2,w3,b3])</span><br><span class="line"></span><br><span class="line">        w1.assign_sub(lr*grads[<span class="number">0</span>])</span><br><span class="line">        b1.assign_sub(lr*grads[<span class="number">1</span>])</span><br><span class="line">        w2.assign_sub(lr*grads[<span class="number">2</span>])</span><br><span class="line">        b2.assign_sub(lr*grads[<span class="number">3</span>])</span><br><span class="line">        w3.assign_sub(lr*grads[<span class="number">4</span>])</span><br><span class="line">        b3.assign_sub(lr*grads[<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(epoch,step,<span class="string">'loss'</span>,float(loss))</span><br><span class="line">            loss_data.append(loss)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[8]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.plot(loss_data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>有几个注意事项:</p>
<ul>
<li><p>tf.GradientTape( )默认只监控由tf.Variable创建的traiable=True属性（默认）的变量:</p>
<p>常用语句:</p>
<ul>
<li><p>with tf.GradientTape( ) as tipe:</p>
</li>
<li><p>tipe.gradient(f,[value])</p>
<blockquote>
<p>对f( value )求导</p>
</blockquote>
</li>
</ul>
</li>
<li><p>tf.data.Dataset.from_tensor_slices((x,y)).batch(128)</p>
<ul>
<li>tf.data.Dataset.from_tensor_slices( )会对最外层分割成单独元素，第0轴</li>
<li>.batch( )方法可以重新将单独元素打包</li>
</ul>
</li>
<li><p>.assign_sub( )方法可以实现tf.Variable创建的变量的自我更新</p>
<blockquote>
<p>因为如果使用w=w-lr*grads ， 经过减法运算后的返回值为tf.tensor , 不为tf.Variable ，会影响下一次循环</p>
</blockquote>
</li>
</ul>
<h1 id="进阶操作"><a href="#进阶操作" class="headerlink" title="进阶操作"></a>进阶操作</h1><h2 id="数据打乱"><a href="#数据打乱" class="headerlink" title="数据打乱"></a>数据打乱</h2><h3 id="tf-random-shuffle-x"><a href="#tf-random-shuffle-x" class="headerlink" title="tf.random.shuffle(x)"></a>tf.random.shuffle(x)</h3><blockquote>
<p>将x中元素顺序打乱</p>
</blockquote>
<h2 id="合并与分割"><a href="#合并与分割" class="headerlink" title="合并与分割"></a>合并与分割</h2><h3 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h3><h4 id="tf-concat-axis"><a href="#tf-concat-axis" class="headerlink" title="tf.concat([,],axis)"></a>tf.concat([,],axis)</h4><blockquote>
<p>按对应轴合并数据</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a=tf.ones([<span class="number">2</span>,<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line">b=tf.ones([<span class="number">3</span>,<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line">c=tf.concat([a,b],axis=<span class="number">0</span>)</span><br><span class="line">c.shape</span><br><span class="line"><span class="comment">#TensorShape([5, 35, 8])</span></span><br></pre></td></tr></table></figure>
<h4 id="tf-stack-axis"><a href="#tf-stack-axis" class="headerlink" title="tf.stack([,],axis)"></a>tf.stack([,],axis)</h4><blockquote>
<p> 可在指定的axis 轴前可以插入一个新的维度,并合并传入的数据</p>
<blockquote>
<p>但对合并数据shape有要求，必须保持一致的shape</p>
</blockquote>
</blockquote>
<h3 id="分割"><a href="#分割" class="headerlink" title="分割"></a>分割</h3><h4 id="tf-unstack-axis"><a href="#tf-unstack-axis" class="headerlink" title="tf.unstack([,],axis)"></a>tf.unstack([,],axis)</h4><blockquote>
<p>可以按轴拆开数据</p>
</blockquote>
<h4 id="tf-split-x-num-or-size-splits-axis"><a href="#tf-split-x-num-or-size-splits-axis" class="headerlink" title="tf.split(x,num_or_size_splits,axis)"></a>tf.split(x,num_or_size_splits,axis)</h4><blockquote>
<ul>
<li><p>x 参数：待分割张量。</p>
</li>
<li><p>num_or_size_splits 参数：切割方案。当num_or_size_splits 为单个数值时，如10，表<br>示等长切割为10 份；当num_or_size_splits 为List 时，List 的每个元素表示每份的长<br>度，如[2,4,2,2]表示切割为4 份，每份的长度依次是2、4、2、2。</p>
</li>
<li><p>axis 参数：指定分割的维度索引号。</p>
</li>
</ul>
</blockquote>
<h2 id="数据统计"><a href="#数据统计" class="headerlink" title="数据统计"></a>数据统计</h2><h3 id="向量范数"><a href="#向量范数" class="headerlink" title="向量范数"></a>向量范数</h3><h4 id="tf-norm-x-ord-axis"><a href="#tf-norm-x-ord-axis" class="headerlink" title="tf.norm(x,ord,axis)"></a>tf.norm(x,ord,axis)</h4><blockquote>
<p>在 TensorFlow 中，可以通过tf.norm(x, ord)求解张量的L1、L2、∞等范数，其中参数<br>ord 指定为1、2 时计算L1、L2 范数，指定为np.inf 时计算∞ −范数。</p>
</blockquote>
<h3 id="最值，均值，和"><a href="#最值，均值，和" class="headerlink" title="最值，均值，和"></a>最值，均值，和</h3><blockquote>
<p>以下函数在没指定axis的值时，默认在所有维度进行判断</p>
</blockquote>
<h4 id="tf-reduce-max-x-axis"><a href="#tf-reduce-max-x-axis" class="headerlink" title="tf.reduce_max(x,axis)"></a>tf.reduce_max(x,axis)</h4><blockquote>
<p>求最大值</p>
</blockquote>
<h4 id="tf-reduce-min-x-axis"><a href="#tf-reduce-min-x-axis" class="headerlink" title="tf.reduce_min(x,axis)"></a>tf.reduce_min(x,axis)</h4><blockquote>
<p>求最小值</p>
</blockquote>
<h4 id="tf-reduce-mean-x-axis"><a href="#tf-reduce-mean-x-axis" class="headerlink" title="tf.reduce_mean(x,axis)"></a>tf.reduce_mean(x,axis)</h4><blockquote>
<p>求均值</p>
</blockquote>
<h4 id="tf-reduce-sum-x-axis"><a href="#tf-reduce-sum-x-axis" class="headerlink" title="tf.reduce_sum(x,axis)"></a>tf.reduce_sum(x,axis)</h4><blockquote>
<p>求和</p>
</blockquote>
<h4 id="tf-argmax-x-axis-和-tf-argmin-x-axis"><a href="#tf-argmax-x-axis-和-tf-argmin-x-axis" class="headerlink" title="tf.argmax(x,axis) 和 tf.argmin(x,axis)"></a>tf.argmax(x,axis) 和 tf.argmin(x,axis)</h4><blockquote>
<p>与numpy中使用方式一致，返回索引</p>
</blockquote>
<h3 id="张量比较"><a href="#张量比较" class="headerlink" title="张量比较"></a>张量比较</h3><h4 id="tf-equal-a-b-和-tf-math-equal-a-b"><a href="#tf-equal-a-b-和-tf-math-equal-a-b" class="headerlink" title="tf.equal(a,b) 和 tf.math.equal(a,b)"></a>tf.equal(a,b) 和 tf.math.equal(a,b)</h4><blockquote>
<p>可以将预测值与真实值比较，返回布尔类型的张量</p>
</blockquote>
<h4 id="tf-unique-x"><a href="#tf-unique-x" class="headerlink" title="tf.unique(x)"></a>tf.unique(x)</h4><blockquote>
<p>返回x的不充分表 和 原来表与不重复表中的索引</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a=tf.constant([<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line">tf.unique(a)</span><br><span class="line"><span class="comment">#Unique(y=&lt;tf.Tensor: id=22, shape=(4,), dtype=int32, numpy=array([3, 4, 1, 0])&gt;,</span></span><br><span class="line"><span class="comment">#idx=&lt;tf.Tensor: id=23, shape=(7,), dtype=int32, numpy=array([0, 1, 1, 2, 2, 2, 3])&gt;)</span></span><br></pre></td></tr></table></figure>
<p>可以使用tf.gather(Unique,idx)完成逆运算</p>
<h2 id="张量排序"><a href="#张量排序" class="headerlink" title="张量排序"></a>张量排序</h2><p>Outline:</p>
<ul>
<li>sort/argsort</li>
<li>Topk</li>
<li>Top-5 Acc.</li>
</ul>
<hr>
<h3 id="tf-sort-x-direction-’ASCENDING’-和-tf-argsort-x-direction-’ASCENDING’"><a href="#tf-sort-x-direction-’ASCENDING’-和-tf-argsort-x-direction-’ASCENDING’" class="headerlink" title="tf.sort(x,direction=’ASCENDING’) 和 tf.argsort(x,direction=’ASCENDING’)"></a>tf.sort(x,direction=’ASCENDING’) 和 tf.argsort(x,direction=’ASCENDING’)</h3><blockquote>
<p>tf.sort( )按照升序或者降序对张量进行排序<br>tf.argsort( )按照升序或者降序对张量进行排序,但返回的是索引</p>
<p>direction=’DESCENDING’ 时为降序</p>
<p>direction=’ASCENDING’ 时为升序</p>
</blockquote>
<h3 id="tf-math-top-k-x-k"><a href="#tf-math-top-k-x-k" class="headerlink" title="tf.math.top_k(x,k)"></a>tf.math.top_k(x,k)</h3><blockquote>
<p>tf.math.top_k(x,k)返回带有前k个最大值的对象</p>
<p>如a=tf.math.top_k(x,k)</p>
<ul>
<li>a.value为带有前k个最大值的张量</li>
<li>a.indices为带有前k个最大值的索引的张量</li>
</ul>
</blockquote>
<hr>
<p>应用:</p>
<ul>
<li>top-k accuracy:<ul>
<li>Prob:[0.1,0.2,0.3,0.4]</li>
<li>Label:[2]</li>
</ul>
</li>
</ul>
<blockquote>
<p>Only consider top-1 prediction: [3]  0%             </p>
<p>Only consider top-2 prediction: [3,2] 100%      </p>
<p>Only consider top-3 prediction: [3,2,1] 100%</p>
</blockquote>
<p>一般会考虑top-5 的预测值去判断模型的性能好坏</p>
<p> Top-k Accuracy: 【仅限二维】</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(output,target,topk=<span class="params">(<span class="number">1</span>,)</span>)</span>:</span></span><br><span class="line">    maxk=max(topk)</span><br><span class="line">    batch_size=target.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    pred=tf.math.top_k(output,maxk).indices</span><br><span class="line">    pred=tf.transpose(pred,[<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    target_=tf.broadcast_to(target,pred.shape)</span><br><span class="line">    correct=tf.equal(pred,target_)</span><br><span class="line">    </span><br><span class="line">    res=[ ]</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> topk:</span><br><span class="line">        correct_k=tf.cast(tf.reshape(correct[:k],[<span class="number">-1</span>]),dtype=tf.float32)</span><br><span class="line">        correct_k=tf.reduce_sum(correct_k)</span><br><span class="line">        acc=float(correct_k/batch_size)</span><br><span class="line">        </span><br><span class="line">     <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<blockquote>
<p>accuracy(output,target,topk=(1,2,3,4,5))</p>
</blockquote>
<h2 id="填充与复制"><a href="#填充与复制" class="headerlink" title="填充与复制"></a>填充与复制</h2><p>Outline</p>
<ul>
<li>pad</li>
<li>tile</li>
<li>broadcast_to</li>
</ul>
<h3 id="tf-pad-tensor-paddings-mode-’CONSTANT’-name-None"><a href="#tf-pad-tensor-paddings-mode-’CONSTANT’-name-None" class="headerlink" title="tf.pad( tensor,paddings,mode=’CONSTANT’,name=None )"></a>tf.pad( tensor,paddings,mode=’CONSTANT’,name=None )</h3><blockquote>
<ul>
<li>tensor是待填充的张量</li>
<li><strong>paddings指出要给tensor的哪个维度进行填充,以及填充方式,要注意的是paddings的rank必须和tensor的rank相同</strong></li>
<li><strong>mode指出用什么进行填充,’CONSTANT’表示用0进行填充(总共有三种填充方式,本文用CONSTANT予以说明pad函数功能)</strong></li>
<li>name就是这个节点的名字</li>
</ul>
<blockquote>
<p>padding=([[ , ],[ , ],[ , ]])</p>
<p>padding的输入大概为这种格式，一个列表表示一个维度的上下左右填充数目且一个列表只有两个元素，表示作用</p>
</blockquote>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a=tf.ones([<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line">tf.pad(a,[[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=25, shape=(6, 10), dtype=float32, numpy=</span></span><br><span class="line"><span class="comment">#array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#       [0., 0., 0., 1., 1., 1., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#       [0., 0., 0., 1., 1., 1., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#       [0., 0., 0., 1., 1., 1., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="tf-tile-tensor"><a href="#tf-tile-tensor" class="headerlink" title="tf.tile(tensor,[ , ,])"></a>tf.tile(tensor,[ , ,])</h3><blockquote>
<p>[ , , ]在里面填每个维度有多少的基础单元tensor，因为必有一个基础单元，所以不能填0</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a=tf.constant([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]])</span><br><span class="line">tf.tile(a,[<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="comment">#&lt;tf.Tensor: id=36, shape=(4, 9), dtype=int32, numpy=</span></span><br><span class="line"><span class="comment">#array([[1, 2, 3, 1, 2, 3, 1, 2, 3],</span></span><br><span class="line"><span class="comment">#       [3, 4, 5, 3, 4, 5, 3, 4, 5],</span></span><br><span class="line"><span class="comment">#       [1, 2, 3, 1, 2, 3, 1, 2, 3],</span></span><br><span class="line"><span class="comment">#       [3, 4, 5, 3, 4, 5, 3, 4, 5]])&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="tf-broadcast-to-tensor"><a href="#tf-broadcast-to-tensor" class="headerlink" title="tf.broadcast_to(tensor,[ , , ])"></a>tf.broadcast_to(tensor,[ , , ])</h3><blockquote>
<p>把tensor尽可能填充成shape[ , , ]</p>
</blockquote>
<h2 id="张量限幅"><a href="#张量限幅" class="headerlink" title="张量限幅"></a>张量限幅</h2><h3 id="tf-maximum-tensor-x-和-tf-minimum-tensor-x"><a href="#tf-maximum-tensor-x-和-tf-minimum-tensor-x" class="headerlink" title="tf.maximum(tensor,x) 和 tf.minimum(tensor,x)"></a>tf.maximum(tensor,x) 和 tf.minimum(tensor,x)</h3><blockquote>
<p>用法同numpy</p>
</blockquote>
<h3 id="tf-clip-by-value-a-min-max"><a href="#tf-clip-by-value-a-min-max" class="headerlink" title="tf.clip_by_value(a,min,max)"></a>tf.clip_by_value(a,min,max)</h3><blockquote>
<p>用法相当于minimum(max,maximum(min,a))</p>
<p>将数据输出限定到一个闭区间内</p>
</blockquote>
<h3 id="tf-clip-by-norm-a-number"><a href="#tf-clip-by-norm-a-number" class="headerlink" title="tf.clip_by_norm(a,number)"></a>tf.clip_by_norm(a,number)</h3><blockquote>
<p>将向量先变成单位向量再乘以number的值，相当于改变向量的模长，不会改变梯度的方向</p>
</blockquote>
<h3 id="tf-clip-by-global-norm-grads-number"><a href="#tf-clip-by-global-norm-grads-number" class="headerlink" title="tf.clip_by_global_norm(grads,number)"></a>tf.clip_by_global_norm(grads,number)</h3><blockquote>
<p>new_grads,total_norm = tf.clip_by_global_norm(grads,number)</p>
<p>梯度裁断（Gradient Clipping）有利于解决梯度爆炸和梯度消失</p>
</blockquote>
<h2 id="高阶操作技巧"><a href="#高阶操作技巧" class="headerlink" title="高阶操作技巧"></a>高阶操作技巧</h2><p>Outline:</p>
<ul>
<li>Where</li>
<li>scatter_nd</li>
<li>mashgrid</li>
</ul>
<hr>
<h3 id="tf-where"><a href="#tf-where" class="headerlink" title="tf.where"></a>tf.where</h3><blockquote>
<p>通过 tf.where(cond, a, b)操作可以根据cond 条件的真假从参数𝑨或𝑩中读取数据，条件<br>判定规则如下：</p>
<script type="math/tex; mode=display">
o_i=\cases{a_i\quad(cond为true) \\ b_i\quad(cond为false)}</script><p>其中𝑖为张量的元素索引，返回的张量大小与𝑨和𝑩一致，当对应位置的cond𝑖为True，𝑜𝑖从<br>𝑎𝑖中复制数据；当对应位置的cond𝑖为False，𝑜𝑖从𝑏𝑖中复制数据。</p>
</blockquote>
<hr>
<blockquote>
<p>但是只传入一个参数时，如tf.where(tensor),会返回非零元素的索引值所构成的tensor</p>
</blockquote>
<h3 id="tf-scatter-nd-indices-updates-shape"><a href="#tf-scatter-nd-indices-updates-shape" class="headerlink" title="tf.scatter_nd(indices,updates,shape)"></a>tf.scatter_nd(indices,updates,shape)</h3><blockquote>
<p>通过 tf.scatter_nd(indices, updates, shape)函数可以高效地刷新张量的部分数据，但是这<br>个函数只能在全0 的白板张量上面执行刷新操作，因此可能需要结合其它操作来实现现有<br>张量的数据刷新功能</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">62</span>]: <span class="comment"># 构造写入位置，即2 个位置</span></span><br><span class="line">indices = tf.constant([[<span class="number">1</span>],[<span class="number">3</span>]])</span><br><span class="line">updates = tf.constant([<span class="comment"># 构造写入数据，即2 个矩阵</span></span><br><span class="line">[[<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>],[<span class="number">6</span>,<span class="number">6</span>,<span class="number">6</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">7</span>,<span class="number">7</span>,<span class="number">7</span>],[<span class="number">8</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">8</span>]],</span><br><span class="line">[[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>]]</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 在shape 为[4,4,4]白板上根据indices 写入updates</span></span><br><span class="line">tf.scatter_nd(indices,updates,[<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line"><span class="comment">#Out[62]:&lt;tf.Tensor: id=477, shape=(4, 4, 4), dtype=int32, numpy=</span></span><br><span class="line"><span class="comment">#array([[[0, 0, 0, 0],</span></span><br><span class="line"><span class="comment">#        [0, 0, 0, 0],</span></span><br><span class="line"><span class="comment">#        [0, 0, 0, 0],</span></span><br><span class="line"><span class="comment">#        [0, 0, 0, 0]],</span></span><br><span class="line"><span class="comment">#       [[5, 5, 5, 5], # 写入的新数据1</span></span><br><span class="line"><span class="comment">#        [6, 6, 6, 6],</span></span><br><span class="line"><span class="comment">#        [7, 7, 7, 7],</span></span><br><span class="line"><span class="comment">#        [8, 8, 8, 8]],</span></span><br><span class="line"><span class="comment">#       [[0, 0, 0, 0],</span></span><br><span class="line"><span class="comment">#        [0, 0, 0, 0],</span></span><br><span class="line"><span class="comment">#        [0, 0, 0, 0],</span></span><br><span class="line"><span class="comment">#        [0, 0, 0, 0]],</span></span><br><span class="line"><span class="comment">#       [[1, 1, 1, 1], # 写入的新数据2</span></span><br><span class="line"><span class="comment">#        [2, 2, 2, 2],</span></span><br><span class="line"><span class="comment">#        [3, 3, 3, 3],</span></span><br><span class="line"><span class="comment">#        [4, 4, 4, 4]]])&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="tf-meshgrid-x-y"><a href="#tf-meshgrid-x-y" class="headerlink" title="tf.meshgrid(x,y)"></a>tf.meshgrid(x,y)</h3><blockquote>
<p>与matlab里的meshgrid用途和操作一致</p>
</blockquote>
<h1 id="数据加载-tenserflow预设数据集"><a href="#数据加载-tenserflow预设数据集" class="headerlink" title="数据加载(tenserflow预设数据集)"></a>数据加载(tenserflow预设数据集)</h1><p>Outlline</p>
<ul>
<li>keras.datasets<ul>
<li>boston housing</li>
<li>mnist/fashion-MNIST dataset</li>
<li>cifar10/100</li>
<li>imdb</li>
</ul>
</li>
<li>tf.data.Dataset.from_tensor_slices<ul>
<li>shuffle</li>
<li>map</li>
<li>batch</li>
<li>repeat</li>
</ul>
</li>
</ul>
<h2 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h2><p>(x,y),(x_test,y_test)=tf.keras.datasets.mnist.load_data( )</p>
<p>tf.one_hot(tensor,depth= )</p>
<hr>
<p>db=tf.data.Dataset.from_tensor_slices((x_test,y_test))</p>
<ul>
<li><p>.shuffle(number)方法</p>
<p>打乱顺序</p>
</li>
<li><p>.map( )方法</p>
<p>数据预处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepocess</span><span class="params">(x,y)</span>:</span></span><br><span class="line">    x=tf.cast(x,dtype=tf.float32)/<span class="number">255</span></span><br><span class="line">    y=tf.cast(y,dtype=tf.int64)/<span class="number">255</span></span><br><span class="line">    y=tf.one_hot(y,depth=<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> x,y</span><br><span class="line">db2=db.map(prepocess)</span><br></pre></td></tr></table></figure>
</li>
<li><p>.batch( )方法</p>
</li>
<li><p>.repeat( )方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">db4=db.repeat(<span class="number">30</span>)</span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> db4:</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<h2 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h2></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepocess</span><span class="params">(x,y)</span>:</span></span><br><span class="line">    x=tf.cast(x,dtype=tf.float32)/<span class="number">255</span></span><br><span class="line">    y=tf.cast(y,dtype=tf.int64)/<span class="number">255</span></span><br><span class="line">    y=tf.one_hot(y,depth=<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> x,y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mnist_dataset</span><span class="params">()</span>:</span></span><br><span class="line">    (x,y),(x_val,y_val)=datasets.fashion_mnist.load_data()</span><br><span class="line">    </span><br><span class="line">    ds=tf.data.Dataset.from_tensor_slices((x,y))</span><br><span class="line">    ds=ds.map(prepocess)</span><br><span class="line">    ds=ds.shuffle(<span class="number">60000</span>).batch(<span class="number">100</span>)</span><br><span class="line">    </span><br><span class="line">    ds_val=tf.data.Dataset.from_tensor_slices((x_val,y-val))</span><br><span class="line">    ds_val=ds_val.map(prepocess)</span><br><span class="line">    ds_val=ds_val.shuffle(<span class="number">10000</span>).batch(<span class="number">100</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> ds,ds_val</span><br></pre></td></tr></table></figure>
<h2 id="测试张量"><a href="#测试张量" class="headerlink" title="测试张量"></a>测试张量</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[1]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># - 可以使用以下方法屏蔽tensorflow无关信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[2]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">'TF_CPP_MIN_LOG_LEVEL'</span>]=<span class="string">'2'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># - 现在加载文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[3]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(x,y),(x_test,y_test)=datasets.mnist.load_data( )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[4]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x=tf.constant(x,dtype=tf.float32)/<span class="number">255</span></span><br><span class="line">y=tf.constant(y,dtype=tf.int32)</span><br><span class="line">x_test=tf.constant(x_test,dtype=tf.float32)/<span class="number">255</span></span><br><span class="line">y_test=tf.constant(y_test,dtype=tf.int32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># - 现在对数据进行切片并打包</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[5]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x,y)).batch(<span class="number">128</span>)</span><br><span class="line">test_db=tf.data.Dataset.from_tensor_slices((x_test,y_test)).batch(<span class="number">128</span>)</span><br><span class="line">loss_data=[]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># - 对权重进行初始化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[6]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">w1=tf.Variable(tf.random.truncated_normal([<span class="number">784</span>,<span class="number">256</span>],stddev=<span class="number">0.1</span>))</span><br><span class="line">b1=tf.Variable(tf.zeros([<span class="number">256</span>]))</span><br><span class="line"></span><br><span class="line">w2=tf.Variable(tf.random.truncated_normal([<span class="number">256</span>,<span class="number">128</span>],stddev=<span class="number">0.1</span>))</span><br><span class="line">b2=tf.Variable(tf.zeros([<span class="number">128</span>]))</span><br><span class="line"></span><br><span class="line">w3=tf.Variable(tf.random.truncated_normal([<span class="number">128</span>,<span class="number">10</span>],stddev=<span class="number">0.1</span>))</span><br><span class="line">b3=tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line">lr=<span class="number">1e-3</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># - 向前传播</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[16]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">50</span>):</span><br><span class="line">    <span class="keyword">for</span> step,(x,y) <span class="keyword">in</span> enumerate(train_db):</span><br><span class="line">        x=tf.reshape(x,[<span class="number">-1</span>,<span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            h1= x@w1 + b1</span><br><span class="line">            h1=tf.nn.relu(h1)</span><br><span class="line"></span><br><span class="line">            h2= h1@w2 + b2</span><br><span class="line">            h2=tf.nn.relu(h2)</span><br><span class="line"></span><br><span class="line">            out= h2@w3 +b3</span><br><span class="line"></span><br><span class="line">            y_onehot=tf.one_hot(y,depth=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">            loss=tf.square(y_onehot - out)</span><br><span class="line">            loss=tf.reduce_mean(loss)</span><br><span class="line"></span><br><span class="line">        grads=tape.gradient(loss,[w1,b1,w2,b2,w3,b3])</span><br><span class="line"></span><br><span class="line">        w1.assign_sub(lr*grads[<span class="number">0</span>])</span><br><span class="line">        b1.assign_sub(lr*grads[<span class="number">1</span>])</span><br><span class="line">        w2.assign_sub(lr*grads[<span class="number">2</span>])</span><br><span class="line">        b2.assign_sub(lr*grads[<span class="number">3</span>])</span><br><span class="line">        w3.assign_sub(lr*grads[<span class="number">4</span>])</span><br><span class="line">        b3.assign_sub(lr*grads[<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(epoch,step,<span class="string">'loss'</span>,float(loss))</span><br><span class="line">            loss_data.append(loss)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[17]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.plot(loss_data)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[18]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">total_correct,total_num=<span class="number">0</span>,<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> step,(x,y) <span class="keyword">in</span> enumerate(test_db):</span><br><span class="line">    x=tf.reshape(x,[<span class="number">-1</span>,<span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line">    </span><br><span class="line">    h1=tf.nn.relu(x@w1+b1)</span><br><span class="line">    h2=tf.nn.relu(h1@w2+b2)</span><br><span class="line">    out=h2@w3+b3</span><br><span class="line">    prob=tf.nn.softmax(out,axis=<span class="number">1</span>)</span><br><span class="line">    preb=tf.argmax(prob,axis=<span class="number">1</span>)</span><br><span class="line">    preb=tf.cast(preb,dtype=tf.int32)</span><br><span class="line">    </span><br><span class="line">    correct=tf.cast(tf.equal(preb,y),dtype=tf.int32)</span><br><span class="line">    correct=tf.reduce_sum(correct)</span><br><span class="line">    </span><br><span class="line">    total_correct+=int(correct)</span><br><span class="line">    total_num+=x.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">acc=total_correct/total_num</span><br><span class="line">print(<span class="string">'test_acc:'</span>,acc)</span><br></pre></td></tr></table></figure>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">prometheus-code</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://prometheus-code.github.io/2020/08/12/tensorflow/tf01/">https://prometheus-code.github.io/2020/08/12/tensorflow/tf01/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://prometheus-code.github.io" target="_blank">司机站</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/">深度学习入门</a><a class="post-meta__tags" href="/tags/tensorflow/">tensorflow</a></div><div class="post_share"><div class="social-share" data-image="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1604255015629&amp;di=b79e0134612b4bc54de608fa8a102d28&amp;imgtype=0&amp;src=http%3A%2F%2Fku.90sjimg.com%2Felement_origin_min_pic%2F16%2F07%2F18%2F17578c9d0e538ba.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/08/12/tensorflow/tf02/"><img class="prev-cover" data-src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1597325218551&amp;di=c155f06387bf214ef2017c6db8da3e20&amp;imgtype=0&amp;src=http%3A%2F%2Fwww.wdshouji.com%2Fwp%2Fwp-content%2Fuploads%2F2019%2F03%2F0a67a5c11f1647bfd324fa0d0f7faab5.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">tensorflow2.0与深度学习入门</div></div></a></div><div class="next-post pull-right"><a href="/2020/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96/"><img class="next-cover" data-src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1595764414107&amp;di=93ee542045570b12290d1eb21dff2e30&amp;imgtype=0&amp;src=http%3A%2F%2F5b0988e595225.cdn.sohucs.com%2Fimages%2F20180319%2F66179109ed054505ba02b054d711dbd1.jpeg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">改善深层神经网络：超参数调试、正则化以及优化</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/09/19/tensorflow/tensorflow异常处理/" title="tensorflow2.0异常处理"><img class="relatedPosts_cover" data-src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1597325218551&di=c155f06387bf214ef2017c6db8da3e20&imgtype=0&src=http%3A%2F%2Fwww.wdshouji.com%2Fwp%2Fwp-content%2Fuploads%2F2019%2F03%2F0a67a5c11f1647bfd324fa0d0f7faab5.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-09-19</div><div class="relatedPosts_title">tensorflow2.0异常处理</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/12/tensorflow/tf02/" title="tensorflow2.0与深度学习入门"><img class="relatedPosts_cover" data-src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1597325218551&di=c155f06387bf214ef2017c6db8da3e20&imgtype=0&src=http%3A%2F%2Fwww.wdshouji.com%2Fwp%2Fwp-content%2Fuploads%2F2019%2F03%2F0a67a5c11f1647bfd324fa0d0f7faab5.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-12</div><div class="relatedPosts_title">tensorflow2.0与深度学习入门</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/12/神经网络/改善深层神经网络：超参数调试、正则化以及优化/" title="改善深层神经网络：超参数调试、正则化以及优化"><img class="relatedPosts_cover" data-src="https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1595764414107&di=93ee542045570b12290d1eb21dff2e30&imgtype=0&src=http%3A%2F%2F5b0988e595225.cdn.sohucs.com%2Fimages%2F20180319%2F66179109ed054505ba02b054d711dbd1.jpeg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-12</div><div class="relatedPosts_title">改善深层神经网络：超参数调试、正则化以及优化</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/10/神经网络/迁移学习/" title="迁移学习"><img class="relatedPosts_cover" data-src="https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=13114550,2928413146&fm=26&gp=0.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-10</div><div class="relatedPosts_title">迁移学习</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/10/神经网络/卷积神经网络/" title="卷积神经网络"><img class="relatedPosts_cover" data-src="https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=13114550,2928413146&fm=26&gp=0.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-10</div><div class="relatedPosts_title">卷积神经网络</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/09/神经网络/与学习相关的技巧/" title="与学习相关的技巧"><img class="relatedPosts_cover" data-src="https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=13114550,2928413146&fm=26&gp=0.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-08-09</div><div class="relatedPosts_title">与学习相关的技巧</div></div></a></div></div></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By prometheus-code</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><button id="readmode" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font_plus" title="放大字体"><i class="fas fa-plus"></i></button><button id="font_minus" title="缩小字体"><i class="fas fa-minus"></i></button><button class="translate_chn_to_cht" id="translateLink" title="简繁转换">繁</button><button id="darkmode" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script></body></html>